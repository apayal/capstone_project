{
  
    
        "post0": {
            "title": "Title",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data_categorical.csv&#39;) . credit_df . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | Female | University Graduate | Married | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | Female | University Graduate | Single | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | Female | University Graduate | Single | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | Female | University Graduate | Married | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | Male | University Graduate | Married | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | Male | High School | Married | 39 | 0 | 0 | 0 | 0 | ... | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | 0 | . 29996 29997 | 150000 | Male | High School | Single | 43 | -1 | -1 | -1 | -1 | ... | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | 0 | . 29997 29998 | 30000 | Male | University Graduate | Single | 37 | 4 | 3 | 2 | -1 | ... | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | 1 | . 29998 29999 | 80000 | Male | High School | Married | 41 | 1 | -1 | 0 | 0 | ... | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | 1 | . 29999 30000 | 50000 | Male | University Graduate | Married | 46 | 0 | 0 | 0 | 0 | ... | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | 1 | . 30000 rows × 25 columns . sns.countplot(credit_df[&#39;default payment next month&#39;],hue=credit_df[&#39;SEX&#39;],palette=&quot;Set2&quot;) plt.xlabel(&#39;Default Payment&#39;) . c: Users Robin anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . Text(0.5, 0, &#39;Default Payment&#39;) . sns.countplot(credit_df[&#39;SEX&#39;],palette=&quot;rocket&quot;) . c: Users Robin anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;SEX&#39;, ylabel=&#39;count&#39;&gt; . sns.countplot(credit_df[&#39;MARRIAGE&#39;],palette=&quot;Set2&quot;) . C: Users Robin anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;MARRIAGE&#39;, ylabel=&#39;count&#39;&gt; . education_percentage=credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 . education_percentage.index . Index([&#39;University Graduate&#39;, &#39;Graduate school&#39;, &#39;High school&#39;, &#39;Unknown&#39;, &#39;other&#39;, &#39;not known&#39;, &#39;not known 1&#39;], dtype=&#39;object&#39;) . plt.figure(figsize=(12,7)) sns.barplot(x=education_percentage.index,y=education_percentage.values,palette=&#39;Set2&#39;) plt.ylabel(&#39;Percentage&#39;) plt.title(&#39;Percentage of Education&#39;) plt.xlabel(&#39;Education&#39;) plt.show() . credit_df[&#39;EDUCATION&#39;].value_counts() . University Graduate 14030 Graduate school 10585 High school 4917 Unknown 280 other institutions 123 Not recognised 51 Not recognised 1 14 Name: EDUCATION, dtype: int64 . # Marriage, Age, and Sex def boxplot_variation(feature1, feature2, feature3, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=feature1, y=feature2, hue=feature3, data=credit_df, palette=&#39;pastel&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_variation(&#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;SEX&#39;,10) . credit_df[&#39;LIMIT_BAL&#39;].value_counts() . 50000 3365 20000 1976 30000 1610 80000 1567 200000 1528 ... 730000 2 1000000 1 327680 1 760000 1 690000 1 Name: LIMIT_BAL, Length: 81, dtype: int64 . plt.figure(figsize = (14,6)) plt.title(&#39;Amount of credit limit - Density Plot&#39;) sns.set_color_codes(&quot;pastel&quot;) sns.distplot(credit_df[&#39;LIMIT_BAL&#39;],kde=True,bins=200) plt.show() . C: Users Robin anaconda3 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) .",
            "url": "https://apayal.github.io/capstone_project/2022/08/04/same.html",
            "relUrl": "/2022/08/04/same.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Predicting credit card defaults",
            "content": "Notebook 2 - EDA . Exploring the data . Table of Contents . Exploratory Data Analysis 1.1 Overview of Data 1.2 Data and Column Descriptions 1.3 Data Dictionary | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data credit_df_dataset_cleaned.csv&#39;) . . Overview of the data . credit_df.head() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 3 | 3 | 0 | 0 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | 0 | 3 | 1 | 1 | 1 | 3 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | 0 | 1 | 0 | 1 | 1 | 1 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 28 columns . credit_df.describe() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 15000.500000 | 167484.322667 | 0.603733 | 35.485500 | 0.983300 | 0.866233 | 0.833800 | 0.779333 | 0.733800 | 0.708900 | ... | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | 0.352833 | 0.467667 | 0.163900 | 0.532133 | 0.012567 | . std 8660.398374 | 129747.661567 | 0.489129 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | 1.133187 | 1.149988 | ... | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | 0.477859 | 0.498962 | 0.370191 | 0.498975 | 0.111396 | . min 1.000000 | 10000.000000 | 0.000000 | 21.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 7500.750000 | 50000.000000 | 0.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 15000.500000 | 140000.000000 | 1.000000 | 34.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | ... | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | . 75% 22500.250000 | 240000.000000 | 1.000000 | 41.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | ... | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | . max 30000.000000 | 1000000.000000 | 1.000000 | 79.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | ... | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 8 rows × 28 columns . Initial observations . Slightly larger number of female&#39;s than male. | Majority of the people are either married or single | Average age of the customers is 35 years old. | Most of the customers are well educated i.e university graduates | Amount of limit balance varies quite a bit as the sd is quite large 0 - will have to look into this more closely later | In general people do not default on thier payments, however we want to focus on those who do default | . . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to see what we are working with . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;AGE&#39;, &#39;PAY_1&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;DEFAULT&#39;, &#39;Education_higher&#39;, &#39;Education_university&#39;, &#39;Education_highschool&#39;, &#39;Marriage_Single&#39;, &#39;Marriage_Other&#39;], dtype=&#39;object&#39;) . Analyse relationship between variables . Correlation . corr = credit_df.corr(method=&#39;spearman&#39;) mask = np.zeros_like(corr,dtype=bool) mask[np.triu_indices_from(mask)]=True cmap = sns.diverging_palette(230, 20, as_cmap=True) with sns.axes_style(&quot;white&quot;): plt.subplots(figsize=(18, 18)) sns.heatmap(corr, mask=mask, square=True, linewidths=.3, fmt=&#39;.2f&#39;, cmap=cmap, annot=True, annot_kws={&quot;size&quot;: 12}) plt.title(&#39;Correlation matrix&#39;, size=15) plt.show() #ref - https://seaborn.pydata.org/examples/many_pairwise_correlations.html . The column that we are most interested in is the default column. Notably with the default column, Pay_1 has the highest correlation followed by the rest of the PAY columns. Limit bal is negatively correlated but has second highest correlation coefficient after the PAY columns. . There are also many correlated columns especially the PAY columns are all highly correlated with each other, as are the BILL_AMT and PAY_AMT. It is likely I will have to perform PCA on this dataset to get rid of multicolinearity. However it will not be needed for all of the different types of models e.g. XGBoost or Decision trees. . Target column - Default . Initial expectations . Looking at the target column we know it only has 2 values - default on payment next month- Yes or no. I am contructing a pie chart to see the distribution of the column. I expect the data to be leaning towards more non-defaulters i.e 0 values. As people do tend to pay thier credit card bills on time usually to avoid late fee&#39;s encurring or debt which can be exorbitant depending on company policy (unless there is a major economic event like recession). . However it is hard to say what percentage of a split we will see as it could be 60:40, 55:45, 70:30 etc. . target = credit_df[&#39;DEFAULT&#39;] . colours = sns.color_palette(&#39;Set2&#39;)[0:5] (credit_df[&#39;DEFAULT&#39;].value_counts(normalize=True)*100).plot(kind=&#39;pie&#39;,autopct=&#39;%.0f%%&#39;,colors=colours,legend=True, figsize=(14,6),startangle=75) # plot using value counts and normalise as it shows us the distribution as a percentage of the whole column plt.legend([&#39;0:Not default&#39;,&#39;1: Default&#39;],loc=&#39;lower right&#39;) plt.title(&#39;Default on payment next month?&#39;) plt.show() . As we know from before that this is a very imbalanced dataset. This is normal as most people will have paid thier credit card bill on time to build good credit history and avoid fees. However I will have to do upsampling on the smaller class to avoid bias to the majority class, which can cause the number of false negatives predicted to be higher. . Demographic observations . The ID column is just an identifier, which also has no predictive value this is something we can remove later when modelling. . Column: LIMIT_BAL . #finding the top values in the column credit_df[&#39;LIMIT_BAL&#39;].value_counts(normalize=True)*100).head(5) . 50000 11.216667 20000 6.586667 30000 5.366667 80000 5.223333 200000 5.093333 Name: LIMIT_BAL, dtype: float64 . credit_df[&#39;LIMIT_BAL&#39;].describe() . count 30000.000000 mean 167484.322667 std 129747.661567 min 10000.000000 25% 50000.000000 50% 140000.000000 75% 240000.000000 max 1000000.000000 Name: LIMIT_BAL, dtype: float64 . The most common credit balance limits are 50k, 20k and 30k respectively. Compared to the minimum 10k and maximum of 1000k, the common figures are on the lower end of the range. However the maximum amount is likely to be an outlier as its more than 2 standard deviations away from the mean. . (credit_df[&#39;LIMIT_BAL&#39;].mean())+(2*credit_df[&#39;LIMIT_BAL&#39;].std()) . 426979.64580105676 . We might need to remove any outliers from this column to avoid getting skewed results. Let&#39;s check the distribution of the limit column first. . limit_log=np.log(credit_df[&#39;LIMIT_BAL&#39;]) #convert the column into log-scale #subplot to compare the difference in distribution plt.subplots(1, 2, figsize = (9, 5)) plt.subplot(1, 2, 1) sns.histplot(data=credit_df[&#39;LIMIT_BAL&#39;],bins=50,kde=True) plt.subplot(1, 2, 2) sns.histplot(limit_log, kde=True) plt.tight_layout() plt.show() . The graph on the left shows the initial distribution of the Limit balance column. As you can see it is quite skewed to the right with one value of credit limit more freqent than the others. However the data is not normally distritubed which can cause problems for models when fitting and predicting - espcecially logistic regression. . On the right is the column that has been log transformed. This gives us a more normally distributed graph although not perfect, should help in increasing accuracy of regression and classification models. . columns = [&#39;AGE&#39;,&#39;PAY_1&#39;,&#39;BILL_AMT1&#39;,&#39;PAY_AMT1&#39;] for col in columns: plt.subplots(1, 2, figsize = (10, 3)) plt.tight_layout() plt.subplot(1, 2, 1) plt.title(f&#39;Original: {col}&#39;) sns.histplot(credit_df[col], bins = 40) plt.subplot(1, 2, 2) plt.title(f&#39;Log transformed: {col}&#39;) sns.histplot(credit_df[col], bins = 40) plt.yscale(&#39;log&#39;) plt.tight_layout() plt.show() . def countplot_comparison(feature1, width): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 5)) s = sns.countplot(ax=ax1, x=target, hue=feature1, data=credit_df, palette=&#39;Set2&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); countplot_comparison(&#39;SEX&#39;,6) . [countplot_comparison(x, 10) for x in credit_df.columns if x.startswith(&#39;E&#39;)] . [None, None, None] . [countplot_comparison(x ,10) for x in credit_df.columns if x.startswith(&#39;M&#39;)] . [None, None] . def boxplot_comparison(feature1, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=target, y=feature1, data=credit_df, palette=&#39;pastel&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_comparison(&#39;LIMIT_BAL&#39;, 10) . boxplot_comparison(&#39;AGE&#39;, 10) . boxplot_comparison(&#39;PAY_1&#39;, 10) . This is where we really see the difference between defaulters and non-defaulters. The non-default column has a very small interquartile range , this is when the middle values are clustered more tightly as well as having close together min and max values. The IQR covers 75% of the data which suggests that the majority of people who don&#39;t default on thier payments tend to be late by only 2 months or 3 at the most barring outliers. . This plot suggests that people who have a higher number of months for thier repayment status column (PAY_1) on average are more likely to default on thier payments. The default values have a higher mediun of about 3, meaning that on average if someone is late on thier payments for 3 months or they will probably default on thier next months payment as well. However the range of values is slightly larger, though there are few outliers, it does show that most defaulters tend to be late by 2-4 months. .",
            "url": "https://apayal.github.io/capstone_project/2022/08/04/EDA.html",
            "relUrl": "/2022/08/04/EDA.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Credit Card Payment Default",
            "content": "Notebook 1: Cleaning Data and Pre-Processing . Steps to load, clean, and analyze the data upon first inspection. . . Table of Contents . Introduction 1.1 Problem Statement 1.2 Data Collection . | Loading &amp; Checking 2.1 Data and Column Descriptions 2.2 Data Dictionary . | Investigate Columns 3.1.Correcting Column names 3.2.Clean unknown values . | Introduction . Business Question . Can we accurately predict the probability of a customer defaulting on their payments? . Data collection . Normally credit risk data is hard to find as it contains private personal details such as marital status, employment status etc. This particular dataset was donated to UCI Machine learning repository, however I initially found it on Kaggle.com. They are both the same but the original on UCI website has a fuller data dictionary of the two. . You can view the original data from here . Dataset Information . This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. . . Importing Libraries . First we must import libraries we need to clean and analyse the data. . import pandas as pd import numpy as np . . Loading &amp; Checking Data . Data and Column Description . Let&#39;s take a closer look at the data, before doing any analysis. . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data.csv&#39;) . print(f&#39;We have {credit_df.shape[0]} rows and {credit_df.shape[1]} columns in the dataset.&#39;) . We have 30000 rows and 25 columns in the dataset. . credit_df.head() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 11 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=0&#39;&gt;1&lt;/a&gt; #a snapshot of the first few rows -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=1&#39;&gt;2&lt;/a&gt; credit_df.head() NameError: name &#39;credit_df&#39; is not defined . credit_df.tail() # last 5 rows . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 29995 29996 | 220000 | 1 | 3 | 1 | 39 | 0 | 0 | 0 | 0 | ... | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | 0 | . 29996 29997 | 150000 | 1 | 3 | 2 | 43 | -1 | -1 | -1 | -1 | ... | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | 0 | . 29997 29998 | 30000 | 1 | 2 | 2 | 37 | 4 | 3 | 2 | -1 | ... | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | 1 | . 29998 29999 | 80000 | 1 | 3 | 1 | 41 | 1 | -1 | 0 | 0 | ... | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | 1 | . 29999 30000 | 50000 | 1 | 2 | 1 | 46 | 0 | 0 | 0 | 0 | ... | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | 1 | . 5 rows × 25 columns . . Data dictionary . Column Name Column Contents . ID&lt;/th&gt; ID of each client&lt;/th&gt; LIMIT_BAL&lt;/th&gt; Amount of given credit in NT dollars (includes individual and family/supplementary credit)&lt;/th&gt; &lt;/p&gt; SEX&lt;/th&gt; Gender (1=male, 2=female)&lt;/th&gt; &lt;/p&gt; EDUCATION&lt;/th&gt; Level of education(1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)&lt;/th&gt; &lt;/p&gt; MARRIAGE&lt;/th&gt; Marital status (1=married, 2=single, 3=others)&lt;/th&gt; &lt;/p&gt; AGE&lt;/th&gt; Age in years&lt;/th&gt; PAY_0&lt;/th&gt; Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)&lt;/th&gt; PAY_2&lt;/th&gt; Repayment status in August, 2005 (scale same as above)&lt;/th&gt; PAY_3&lt;/th&gt; Repayment status in July, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; PAY_4&lt;/th&gt; Repayment status in June, 2005 (scale same as above)&lt;/th&gt; PAY_5&lt;/th&gt; Repayment status in May, 2005 (scale same as above)&lt;/th&gt; PAY_6&lt;/th&gt; Repayment status in April, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; BILL_AMT1&lt;/th&gt; Amount of bill statement in September, 2005 (NT dollar)&lt;/th&gt; &lt;/p&gt; BILL_AMT2&lt;/th&gt; Amount of bill statement in August, 2005 (NT dollar)&lt;/th&gt; BILL_AMT3&lt;/th&gt; Amount of bill statement in July, 2005 (NT dollar)&lt;/th&gt; BILL_AMT4&lt;/th&gt; Amount of bill statement in June, 2005 (NT dollar)&lt;/th&gt; BILL_AMT5&lt;/th&gt; Amount of bill statement in May, 2005 (NT dollar)&lt;/th&gt; BILL_AMT6&lt;/th&gt; Amount of bill statement in April, 2005 (NT dollar)&lt;/th&gt; PAY_AMT1&lt;/th&gt; Amount of previous payment in September, 2005 (NT dollar)&lt;/th&gt; PAY_AMT2&lt;/th&gt; Amount of previous payment in August, 2005 (NT dollar)&lt;/th&gt; PAY_AMT3&lt;/th&gt; Amount of previous payment in July, 2005 (NT dollar)&lt;/th&gt; PAY_AMT4&lt;/th&gt; Amount of previous payment in June, 2005 (NT dollar)&lt;/th&gt; PAY_AMT5&lt;/th&gt; Amount of previous payment in May, 2005 (NT dollar)&lt;/th&gt; PAY_AMT6&lt;/th&gt; Amount of previous payment in April, 2005 (NT dollar)&lt;/th&gt; default.payment.next.month&lt;/th&gt; Default payment (1=yes, 0=no)&lt;/th&gt; &lt;/table&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Data cleaning and preprocessing . Dataset Summary Information . credit_df.isna().sum() . ID 0 LIMIT_BAL 0 SEX 0 EDUCATION 0 MARRIAGE 0 AGE 0 PAY_0 0 PAY_2 0 PAY_3 0 PAY_4 0 PAY_5 0 PAY_6 0 BILL_AMT1 0 BILL_AMT2 0 BILL_AMT3 0 BILL_AMT4 0 BILL_AMT5 0 BILL_AMT6 0 PAY_AMT1 0 PAY_AMT2 0 PAY_AMT3 0 PAY_AMT4 0 PAY_AMT5 0 PAY_AMT6 0 default payment next month 0 dtype: int64 . credit_df.duplicated().sum() . 0 . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to check all the column names are appropriate . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;PAY_0&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;default payment next month&#39;], dtype=&#39;object&#39;) . The columns PAY_0-PAY_6 are not very intuitive columns names. These columns represent the repayment history of the customer, so we can rename them to something like HIST_PAY1. The sixth column starts at PAY_0 but then skips to PAY_2. Looking at the other columns in a group such as BILL_AMT and PAY_AMT they all start at 1 and go up to 6. So we need to rename the column PAY_0 as PAY_1 so it has the same format as the other columns and avoid confusion. . Default payment next month is the target column i.e the one we want to predict. This column name should also be changed for easier access,reference and most importantly so it follows the same uniform naming convention as the other columns - 1 or 2 words in capitals that succinctly describes the data contained in the column. . new_column_names = {&#39;PAY_0&#39;: &#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;DEFAULT&#39; } . credit_df.rename(columns=new_column_names,inplace=True) . credit_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30000 entries, 0 to 29999 Data columns (total 25 columns): # Column Non-Null Count Dtype -- -- 0 ID 30000 non-null int64 1 LIMIT_BAL 30000 non-null int64 2 SEX 30000 non-null int64 3 EDUCATION 30000 non-null int64 4 MARRIAGE 30000 non-null int64 5 AGE 30000 non-null int64 6 PAY_1 30000 non-null int64 7 PAY_2 30000 non-null int64 8 PAY_3 30000 non-null int64 9 PAY_4 30000 non-null int64 10 PAY_5 30000 non-null int64 11 PAY_6 30000 non-null int64 12 BILL_AMT1 30000 non-null int64 13 BILL_AMT2 30000 non-null int64 14 BILL_AMT3 30000 non-null int64 15 BILL_AMT4 30000 non-null int64 16 BILL_AMT5 30000 non-null int64 17 BILL_AMT6 30000 non-null int64 18 PAY_AMT1 30000 non-null int64 19 PAY_AMT2 30000 non-null int64 20 PAY_AMT3 30000 non-null int64 21 PAY_AMT4 30000 non-null int64 22 PAY_AMT5 30000 non-null int64 23 PAY_AMT6 30000 non-null int64 24 DEFAULT 30000 non-null int64 dtypes: int64(25) memory usage: 5.7 MB . . Pre-Processing . Lets focus on cleaning and checking for inconsistencies in the demographic columns of the dataset first. . Column: SEX . From the data dictionary we already know that sex has the values 1 and 2 representing male and female. However in computing binary values are traditionally represented as 0 and 1, so we will change them. . credit_df[&#39;SEX&#39;].unique() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 25 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000025?line=0&#39;&gt;1&lt;/a&gt; credit_df[&#39;SEX&#39;].unique() NameError: name &#39;credit_df&#39; is not defined . credit_df[&#39;SEX&#39;]=credit_df[&#39;SEX&#39;].replace({1: 0, 2: 1}) . credit_df[&#39;SEX&#39;].value_counts() # check we only have 0 and 1 left . 1 18112 0 11888 Name: SEX, dtype: int64 . . Column: Education . Education also has some extra values. We already know there is 2 redundent values of 5 and 6. We have 4 to denote &#39;unknown&#39; or &#39;other&#39; education status. We can do a value counts to check if there are additional unknown values and if there are then we can also assign them to 4. . credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 . 2 46.766667 1 35.283333 3 16.390000 5 0.933333 4 0.410000 6 0.170000 0 0.046667 Name: EDUCATION, dtype: float64 . credit_df[&#39;EDUCATION&#39;]=credit_df[&#39;EDUCATION&#39;].replace([0,5,6], 4 ) credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 # now do get dummies so we only have 3 columns left . 2 46.766667 1 35.283333 3 16.390000 4 1.560000 Name: EDUCATION, dtype: float64 . There is actually 3 unknown values: 0,5,6. We can map these to the value 4 to refer to other either instituations or other unknown place of study. . However upon doing some external reseach, I found that Taiwan has a very high percentage of education completion in general. In fact around 94.7% of people have passed high school and gone onto some sort of higher education or senior vocational course . Based on this information, it is safe to assume that the unknown values and other category can be grouped together in the value 3, as it is highly likely that these people have passed high school at very the least. Additionally since adding the 3 unknown values to 4 gives us 468 values which accounts for less than 1.56% of the entire column it is insignificant in making predictions which means it can only contribute to making models more computationally expensive to compute. . education_df = pd.get_dummies(credit_df[&#39;EDUCATION&#39;],prefix=&#39;EDUCATION&#39;) . education_df.drop(columns=[&#39;EDUCATION_4&#39;], inplace=True) . education_df . Education_1 Education_2 Education_3 . 0 0 | 1 | 0 | . 1 0 | 1 | 0 | . 2 0 | 1 | 0 | . 3 0 | 1 | 0 | . 4 0 | 1 | 0 | . ... ... | ... | ... | . 29995 0 | 0 | 1 | . 29996 0 | 0 | 1 | . 29997 0 | 1 | 0 | . 29998 0 | 0 | 1 | . 29999 0 | 1 | 0 | . 30000 rows × 3 columns . education_df.value_counts() . Education_1 Education_2 Education_3 0 1 0 14030 1 0 0 10585 0 0 1 4917 0 468 dtype: int64 . . Column: Marriage . This also has 1 unknown value of 0, which we can change to 3 (others) . credit_df[&#39;MARRIAGE&#39;].value_counts() . 2 15964 1 13659 3 323 0 54 Name: MARRIAGE, dtype: int64 . credit_df[&#39;MARRIAGE&#39;]=credit_df[&#39;MARRIAGE&#39;].replace(0,3) . credit_df[&#39;MARRIAGE&#39;].value_counts(normalize=True)*100 . 2 53.213333 1 45.530000 3 1.256667 Name: MARRIAGE, dtype: float64 . Married and single can essentially be represented as 1 columns of married or not. Although value 3 only accounts for 1.25% of the column we can at the moment have a separate column for it but later when we do PCA or feature engineering I suspect it will be unimportant in predictions ans thus will be removed or not included in the modelling. . credit_df[&#39;MARRIAGE&#39;] . 0 1 1 2 2 2 3 1 4 1 .. 29995 1 29996 2 29997 2 29998 1 29999 1 Name: MARRIAGE, Length: 30000, dtype: int64 . marriage_df=pd.get_dummies(credit_df[&#39;MARRIAGE&#39;],prefix=&#39;Marital_status&#39;,drop_first=True) . marriage_df . Marital_status_2 Marital_status_3 . 0 0 | 0 | . 1 1 | 0 | . 2 1 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 29995 0 | 0 | . 29996 1 | 0 | . 29997 1 | 0 | . 29998 0 | 0 | . 29999 0 | 0 | . 30000 rows × 2 columns . We have removed the first column as the second column represents 0 for Married people and 1 for sinlge people. Maritial status_3 represents any another values. . . Putting back the cleaned data into 1 DataFrame . credit_df_clean = pd.concat([credit_df,education_df, marriage_df], axis=1) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_1 Education_2 Education_3 Marital_status_2 Marital_status_3 . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . rename_col = {&#39;Education_1&#39;: &#39;Education_higher&#39;, &#39;Education_2&#39;:&#39;Education_university&#39;, &#39;Education_3&#39;:&#39;Education_highschool&#39;, &#39;Marital_status_2&#39;:&#39;Marriage_Single&#39;, &#39;Marital_status_3&#39;:&#39;Marriage_Other&#39; } . credit_df_clean.rename(columns=rename_col,inplace=True) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . credit_df_clean=credit_df_clean.drop(columns=[&#39;EDUCATION&#39;,&#39;MARRIAGE&#39;]) . credit_df_clean . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5003 | 3047 | 5000 | 1000 | 0 | 0 | 0 | 1 | 0 | 0 | . 29996 29997 | 150000 | 0 | 43 | -1 | -1 | -1 | -1 | 0 | 0 | ... | 8998 | 129 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 29997 29998 | 30000 | 0 | 37 | 4 | 3 | 2 | -1 | 0 | 0 | ... | 22000 | 4200 | 2000 | 3100 | 1 | 0 | 1 | 0 | 1 | 0 | . 29998 29999 | 80000 | 0 | 41 | 1 | -1 | 0 | 0 | 0 | -1 | ... | 1178 | 1926 | 52964 | 1804 | 1 | 0 | 0 | 1 | 0 | 0 | . 29999 30000 | 50000 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1430 | 1000 | 1000 | 1000 | 1 | 0 | 1 | 0 | 0 | 0 | . 30000 rows × 28 columns . . Column: PAY columns values . The PAY_ columns range is actually -2 to 8 whereas in the dictionary it said -1 to 9. Let us make it 0-10 instead so all values are positive but the range and what it represents is unchanged. . credit_df_pay_col=credit_df_clean.columns[4:10] for month in credit_df_pay_col: print(sorted(credit_df_clean[month].unique())) . for month in credit_df_clean.columns[4:10]: credit_df_clean[month]=credit_df_clean[month]+2 # inplace=True) credit_df_clean[&#39;PAY_1&#39;].value_counts() . credit_df_clean.head() . . Saving the work . #credit_df_clean.to_csv(&#39;credit_df_dataset_cleaned.csv&#39;, index=False) # the index column will not be saved . . Ending notes . In this notebook, I have mainly changed column names to more suitable names, reassign unknown values in column. Much of the data was originally categorical data that was transformed into ascending order values. However as this could be misinterpreted as values are ordinal. So re-encoding the categorical values into binary 0 and 1 was essential. . The next steps are to start doing some inital EDA on the dataset to see what initial insights we can gain. . . &lt;/div&gt; . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | .",
            "url": "https://apayal.github.io/capstone_project/2022/08/04/Data_cleaning_pre-processing.html",
            "relUrl": "/2022/08/04/Data_cleaning_pre-processing.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Models",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm from imblearn.over_sampling import SMOTE #Model Building Libraries from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from xgboost import XGBClassifier from sklearn.metrics import accuracy_score,classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve #Dimensionality Reduction from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler sc = StandardScaler() #Cross-Validation from sklearn.model_selection import cross_val_score #settings pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, None) import warnings warnings.filterwarnings(&quot;ignore&quot;) . c: Users Robin anaconda3 lib site-packages xgboost compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. from pandas import MultiIndex, Int64Index . credit_df = pd.read_csv(&quot;credit-df-dataset-cleaned.csv&quot;) credit_df.head() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 3 | 3 | 0 | 0 | -1 | -1 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | 0 | 3 | 1 | 1 | 1 | 3 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 1 | 1 | 1 | 1 | 1 | 1 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 1 | 1 | 1 | 1 | 1 | 1 | 46990 | 48233 | 49291 | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | 0 | 1 | 0 | 1 | 1 | 1 | 8617 | 5670 | 35835 | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . Y = credit_df[:][&#39;DEFAULT&#39;] X = credit_df.drop([&#39;DEFAULT&#39;,&#39;ID&#39;], axis=1) . X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) # describes info about train and test set print(&quot;Number transactions X_train dataset: &quot;, X_train.shape) print(&quot;Number transactions y_train dataset: &quot;, y_train.shape) print(&quot;Number transactions X_test dataset: &quot;, X_test.shape) print(&quot;Number transactions y_test dataset: &quot;, y_test.shape) . Number transactions X_train dataset: (24000, 26) Number transactions y_train dataset: (24000,) Number transactions X_test dataset: (6000, 26) Number transactions y_test dataset: (6000,) . print(&quot;Before OverSampling, counts of label &#39;1&#39;: {}&quot;.format(sum(y_train == 1))) print(&quot;Before OverSampling, counts of label &#39;0&#39;: {} n&quot;.format(sum(y_train == 0))) sm = SMOTE(random_state = 2) X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel()) print(&#39;After OverSampling, the shape of train_X: {}&#39;.format(X_train_res.shape)) print(&#39;After OverSampling, the shape of train_y: {} n&#39;.format(y_train_res.shape)) print(&quot;After OverSampling, counts of label &#39;1&#39;: {}&quot;.format(sum(y_train_res == 1))) print(&quot;After OverSampling, counts of label &#39;0&#39;: {}&quot;.format(sum(y_train_res == 0))) . Before OverSampling, counts of label &#39;1&#39;: 5339 Before OverSampling, counts of label &#39;0&#39;: 18661 After OverSampling, the shape of train_X: (37322, 26) After OverSampling, the shape of train_y: (37322,) After OverSampling, counts of label &#39;1&#39;: 18661 After OverSampling, counts of label &#39;0&#39;: 18661 . X_train_res.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 20000 | 1 | 44 | 1 | 1 | 3 | 1 | 1 | -1 | 17095 | 19112 | 17980 | 18780 | 0 | 0 | 3000 | 0 | 1000 | 1000 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 260000 | 1 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 165 | 274 | 165 | 333 | 165 | 165 | 274 | 165 | 333 | 165 | 293 | 0 | 1 | 0 | 1 | 0 | . 2 20000 | 0 | 39 | 3 | 1 | 1 | 1 | 1 | 1 | 20264 | 20328 | 19299 | 19928 | 20204 | 20398 | 1500 | 1500 | 900 | 700 | 1480 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 30000 | 0 | 23 | 3 | 3 | 3 | 3 | 3 | 3 | 28224 | 29276 | 28635 | 30127 | 30525 | 29793 | 1800 | 150 | 2250 | 1000 | 0 | 700 | 0 | 1 | 0 | 0 | 0 | . 4 10000 | 0 | 29 | 1 | 1 | 1 | 1 | 1 | 1 | 8275 | 8409 | 8600 | 9470 | 6690 | 9690 | 2800 | 2000 | 1500 | 900 | 3000 | 0 | 0 | 1 | 0 | 0 | 0 | . print(&quot;Number transactions X_train dataset: &quot;, X_train_res.shape) print(&quot;Number transactions y_train dataset: &quot;, y_train_res.shape) print(&quot;Number transactions X_test dataset: &quot;, X_test.shape) print(&quot;Number transactions y_test dataset: &quot;, y_test.shape) . Number transactions X_train dataset: (37322, 26) Number transactions y_train dataset: (37322,) Number transactions X_test dataset: (6000, 26) Number transactions y_test dataset: (6000,) . lr=LogisticRegression() knc = KNeighborsClassifier() svc = SVC() xgb = XGBClassifier() #to store results models = [&#39;Logistic Regression&#39;, &quot;KNN&quot;, &quot;SVM&quot;, &quot;XGBoost&quot;] recall = [] accuracy = [] precision = [] f1 = [] . lr.get_params() . {&#39;C&#39;: 1.0, &#39;class_weight&#39;: None, &#39;dual&#39;: False, &#39;fit_intercept&#39;: True, &#39;intercept_scaling&#39;: 1, &#39;l1_ratio&#39;: None, &#39;max_iter&#39;: 100, &#39;multi_class&#39;: &#39;auto&#39;, &#39;n_jobs&#39;: None, &#39;penalty&#39;: &#39;l2&#39;, &#39;random_state&#39;: None, &#39;solver&#39;: &#39;lbfgs&#39;, &#39;tol&#39;: 0.0001, &#39;verbose&#39;: 0, &#39;warm_start&#39;: False} . knc.get_params() . {&#39;algorithm&#39;: &#39;auto&#39;, &#39;leaf_size&#39;: 30, &#39;metric&#39;: &#39;minkowski&#39;, &#39;metric_params&#39;: None, &#39;n_jobs&#39;: None, &#39;n_neighbors&#39;: 5, &#39;p&#39;: 2, &#39;weights&#39;: &#39;uniform&#39;} . svc.get_params() . {&#39;C&#39;: 1.0, &#39;break_ties&#39;: False, &#39;cache_size&#39;: 200, &#39;class_weight&#39;: None, &#39;coef0&#39;: 0.0, &#39;decision_function_shape&#39;: &#39;ovr&#39;, &#39;degree&#39;: 3, &#39;gamma&#39;: &#39;scale&#39;, &#39;kernel&#39;: &#39;rbf&#39;, &#39;max_iter&#39;: -1, &#39;probability&#39;: False, &#39;random_state&#39;: None, &#39;shrinking&#39;: True, &#39;tol&#39;: 0.001, &#39;verbose&#39;: False} . Logistic Regression . lr = lr.fit(X_train_res, y_train_res) y_pred = lr.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_lrc = round(accuracy_score(y_test,y_pred),3) recall_lr = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_lrc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_lr = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy.append(accuracy_lrc) recall.append(recall_lr) precision.append(precision_lrc) f1.append(f1_score_lr) . precision recall f1-score support 0 0.83 0.70 0.76 4703 1 0.31 0.49 0.38 1297 accuracy 0.65 6000 macro avg 0.57 0.59 0.57 6000 weighted avg 0.72 0.65 0.68 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . Log Transformation . It is possible that some numerical variables could be skewed in nature. Log Transformation is one of the technique that can help in removing the skewness from data by making the data Normally Distributed. . Numerical Variables: &#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1&#39;,&#39;BILL_AMT2&#39;,&#39;BILL_AMT3&#39;,&#39;BILL_AMT4&#39;,&#39;BILL_AMT5&#39;,&#39;BILL_AMT6&#39; could be skewed in the nature. . X_train_log = X_train_res.copy() X_train_log.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 20000 | 1 | 44 | 1 | 1 | 3 | 1 | 1 | -1 | 17095 | 19112 | 17980 | 18780 | 0 | 0 | 3000 | 0 | 1000 | 1000 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 260000 | 1 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 165 | 274 | 165 | 333 | 165 | 165 | 274 | 165 | 333 | 165 | 293 | 0 | 1 | 0 | 1 | 0 | . 2 20000 | 0 | 39 | 3 | 1 | 1 | 1 | 1 | 1 | 20264 | 20328 | 19299 | 19928 | 20204 | 20398 | 1500 | 1500 | 900 | 700 | 1480 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 30000 | 0 | 23 | 3 | 3 | 3 | 3 | 3 | 3 | 28224 | 29276 | 28635 | 30127 | 30525 | 29793 | 1800 | 150 | 2250 | 1000 | 0 | 700 | 0 | 1 | 0 | 0 | 0 | . 4 10000 | 0 | 29 | 1 | 1 | 1 | 1 | 1 | 1 | 8275 | 8409 | 8600 | 9470 | 6690 | 9690 | 2800 | 2000 | 1500 | 900 | 3000 | 0 | 0 | 1 | 0 | 0 | 0 | . fig, axes = plt.subplots(7, 2, figsize=(30,50)) sns.distplot(X_train_log[&#39;LIMIT_BAL&#39;],ax=axes[0,0]) sns.distplot(X_train_log[&#39;AGE&#39;],ax=axes[0,1]) sns.distplot(X_train_log[&#39;BILL_AMT1&#39;],ax=axes[1,0]) sns.distplot(X_train_log[&#39;BILL_AMT2&#39;],ax=axes[1,1]) sns.distplot(X_train_log[&#39;BILL_AMT3&#39;],ax=axes[2,0]) sns.distplot(X_train_log[&#39;BILL_AMT4&#39;],ax=axes[2,1]) sns.distplot(X_train_log[&#39;BILL_AMT5&#39;],ax=axes[3,0]) sns.distplot(X_train_log[&#39;BILL_AMT6&#39;],ax=axes[3,1]) sns.distplot(X_train_log[&#39;PAY_AMT1&#39;],ax=axes[4,0]) sns.distplot(X_train_log[&#39;PAY_AMT2&#39;],ax=axes[4,1]) sns.distplot(X_train_log[&#39;PAY_AMT3&#39;],ax=axes[5,0]) sns.distplot(X_train_log[&#39;PAY_AMT4&#39;],ax=axes[5,1]) sns.distplot(X_train_log[&#39;PAY_AMT5&#39;],ax=axes[6,0]) sns.distplot(X_train_log[&#39;PAY_AMT6&#39;],ax=axes[6,1]) . &lt;AxesSubplot:xlabel=&#39;PAY_AMT6&#39;, ylabel=&#39;Density&#39;&gt; . There is not much skewness in Pay columns so as of now will be focussin on [&#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1&#39;,&#39;BILL_AMT2&#39;,&#39;BILL_AMT3&#39;,&#39;BILL_AMT4&#39;,&#39;BILL_AMT5&#39;,&#39;BILL_AMT6&#39;&#39;PAY_AMT1&#39;,&#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;,&#39;PAY_AMT4&#39;,&#39;PAY_AMT5&#39;,&#39;PAY_AMT6&#39;] columns only. . Log Transformation of negative values and 0 is undefined so first will be converting all negative values to positive by adding a same constant in all the values. . constant = 400000 col = [&#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1&#39;,&#39;BILL_AMT2&#39;,&#39;BILL_AMT3&#39;,&#39;BILL_AMT4&#39;,&#39;BILL_AMT5&#39;,&#39;BILL_AMT6&#39;,&#39;PAY_AMT1&#39;,&#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;,&#39;PAY_AMT4&#39;,&#39;PAY_AMT5&#39;,&#39;PAY_AMT6&#39;] for columns in col: X_train_log[columns] = X_train_log[columns]+constant X_train_log[columns] = np.log10(X_train_log[columns]) . X_train_log.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 5.623249 | 1 | 5.602108 | 1 | 1 | 3 | 1 | 1 | -1 | 5.620235 | 5.622330 | 5.621156 | 5.621986 | 5.602060 | 5.602060 | 5.605305 | 5.602060 | 5.603144 | 5.603144 | 5.602060 | 5.602060 | 0 | 0 | 1 | 0 | 0 | . 1 5.819544 | 1 | 5.602093 | 0 | 0 | 0 | 0 | 0 | 0 | 5.602239 | 5.602239 | 5.602357 | 5.602239 | 5.602421 | 5.602239 | 5.602239 | 5.602357 | 5.602239 | 5.602421 | 5.602239 | 5.602378 | 0 | 1 | 0 | 1 | 0 | . 2 5.623249 | 0 | 5.602102 | 3 | 1 | 1 | 1 | 1 | 1 | 5.623522 | 5.623588 | 5.622524 | 5.623175 | 5.623460 | 5.623661 | 5.603686 | 5.603686 | 5.603036 | 5.602819 | 5.603664 | 5.602060 | 0 | 1 | 0 | 0 | 0 | . 3 5.633468 | 0 | 5.602085 | 3 | 3 | 3 | 3 | 3 | 3 | 5.631671 | 5.632737 | 5.632088 | 5.633597 | 5.633998 | 5.633259 | 5.604010 | 5.602223 | 5.604496 | 5.603144 | 5.602060 | 5.602819 | 0 | 1 | 0 | 0 | 0 | . 4 5.612784 | 0 | 5.602091 | 1 | 1 | 1 | 1 | 1 | 1 | 5.610953 | 5.611095 | 5.611298 | 5.612222 | 5.609263 | 5.612455 | 5.605089 | 5.604226 | 5.603686 | 5.603036 | 5.605305 | 5.602060 | 0 | 1 | 0 | 0 | 0 | . lr = lr.fit(X_train_log, y_train_res) y_pred = lr.predict(X_test) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.78 1.00 0.88 4703 1 0.25 0.00 0.00 1297 accuracy 0.78 6000 macro avg 0.52 0.50 0.44 6000 weighted avg 0.67 0.78 0.69 6000 . Transforming into log is giving recall value for label 1 as 0.00, which means it is predicting all the classes as 0. It is performing poorly so will be avoiding log transformation in further steps. . KNN . knc = knc.fit(X_train_res, y_train_res) y_pred = knc.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_knc = round(accuracy_score(y_test,y_pred),3) recall_knc = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_knc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_knc = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy.append(accuracy_knc) recall.append(recall_knc) precision.append(precision_knc) f1.append(f1_score_knc) . precision recall f1-score support 0 0.83 0.62 0.71 4703 1 0.28 0.54 0.37 1297 accuracy 0.60 6000 macro avg 0.56 0.58 0.54 6000 weighted avg 0.71 0.60 0.64 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . SVM . svc = svc.fit(X_train_res, y_train_res) y_pred = svc.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_svc = round(accuracy_score(y_test,y_pred),3) recall_svc = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_svc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_svc = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy.append(accuracy_svc) recall.append(recall_svc) precision.append(precision_svc) f1.append(f1_score_svc) . precision recall f1-score support 0 0.87 0.54 0.66 4703 1 0.29 0.70 0.41 1297 accuracy 0.57 6000 macro avg 0.58 0.62 0.54 6000 weighted avg 0.74 0.57 0.61 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . XGBoost . xgb = xgb.fit(X_train_res, y_train_res) y_pred = xgb.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_xgb = round(accuracy_score(y_test,y_pred),3) recall_xgb = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_xgb = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_xgb = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy.append(accuracy_xgb) recall.append(recall_xgb) precision.append(precision_xgb) f1.append(f1_score_xgb) . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Credit_Card_Default_modelling.ipynb Cell 32 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Credit_Card_Default_modelling.ipynb#X43sZmlsZQ%3D%3D?line=0&#39;&gt;1&lt;/a&gt; xgb = xgb.fit(X_train_res, y_train_res) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Credit_Card_Default_modelling.ipynb#X43sZmlsZQ%3D%3D?line=1&#39;&gt;2&lt;/a&gt; y_pred = xgb.predict(X_test) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Credit_Card_Default_modelling.ipynb#X43sZmlsZQ%3D%3D?line=2&#39;&gt;3&lt;/a&gt; print(classification_report(y_test, y_pred)) NameError: name &#39;xgb&#39; is not defined . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . dataframe = pd.DataFrame(list(zip(models, accuracy, recall, precision, f1)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) dataframe.head() . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.651 | 0.651 | 0.718 | 0.675 | . 1 KNN | 0.605 | 0.605 | 0.711 | 0.638 | . 2 SVM | 0.571 | 0.571 | 0.743 | 0.608 | . 3 XGBoost | 0.777 | 0.777 | 0.768 | 0.772 | . GridSearchCV . logistic_parameters = {&#39;solver&#39;:(&#39;sag&#39;, &#39;saga&#39;,&#39;newton-cg&#39;,&#39;liblinear&#39;,&#39;lbfgs&#39;),&quot;penalty&quot;:(&quot;l1&quot;,&quot;l2&quot;,&quot;None&quot;)} knn_parameters = {&#39;n_neighbors&#39;:(range(5,10)), &quot;algorithm&quot;:(&quot;auto&quot;,&quot;kd_tree&quot;)} svm_parameters = {&quot;gamma&quot;:(1, 0.1, 0.01),&quot;kernel&quot;:(&quot;linear&quot;,&quot;rbf&quot;)} xgboost_parameters = {&quot;n_estimators&quot;: (45, 50, 55)} . parameter_dictionary = {} #function to find best parameters for each model using GridSearchCV def optimize_params(model, parameter_list): #taking only a sub-sample of training data so to reduce the timing. X_sub = X_train_res[0:10000] y_sub = y_train_res[0:10000] grid = GridSearchCV(model, parameter_list, cv=3) grid.fit(X_sub, y_sub) parameter_dictionary[model] = grid.best_params_ . # optimize_params(lr, logistic_parameters) # optimize_params(knc, knn_parameters) # optimize_params(svc, svm_parameters) # optimize_params(xgb, xgboost_parameters) . . lr=LogisticRegression(solver=&#39;liblinear&#39;, penalty=&#39;l1&#39;) knc = KNeighborsClassifier(n_neighbors = 6, algorithm=&#39;kd_tree&#39;) svc = SVC(kernel=&quot;rbf&quot;, gamma=1) xgb = XGBClassifier(n_estimators = 50) . recall_best_params = [] accuracy_best_params = [] precision_best_params = [] f1_best_params = [] . Logistic Regression . lr = lr.fit(X_train_res, y_train_res) y_pred = lr.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_lrc = round(accuracy_score(y_test,y_pred),3) recall_lr = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_lrc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_lr = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy_best_params.append(accuracy_lrc) recall_best_params.append(recall_lr) precision_best_params.append(precision_lrc) f1_best_params.append(f1_score_lr) . precision recall f1-score support 0 0.84 0.85 0.85 4703 1 0.44 0.42 0.43 1297 accuracy 0.76 6000 macro avg 0.64 0.63 0.64 6000 weighted avg 0.75 0.76 0.76 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . KNN . knc = knc.fit(X_train_res, y_train_res) y_pred = knc.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_knc = round(accuracy_score(y_test,y_pred),3) recall_knc = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_knc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_knc = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy_best_params.append(accuracy_knc) recall_best_params.append(recall_knc) precision_best_params.append(precision_knc) f1_best_params.append(f1_score_knc) . precision recall f1-score support 0 0.82 0.69 0.75 4703 1 0.29 0.46 0.35 1297 accuracy 0.64 6000 macro avg 0.56 0.57 0.55 6000 weighted avg 0.71 0.64 0.66 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . SVM . svc = svc.fit(X_train_res, y_train_res) y_pred = svc.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_svc = round(accuracy_score(y_test,y_pred),3) recall_svc = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_svc = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_svc = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy_best_params.append(accuracy_svc) recall_best_params.append(recall_svc) precision_best_params.append(precision_svc) f1_best_params.append(f1_score_svc) . precision recall f1-score support 0 0.79 0.99 0.88 4703 1 0.47 0.02 0.04 1297 accuracy 0.78 6000 macro avg 0.63 0.51 0.46 6000 weighted avg 0.72 0.78 0.70 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . XGBoost . xgb = xgb.fit(X_train_res, y_train_res) y_pred = xgb.predict(X_test) print(classification_report(y_test, y_pred)) accuracy_xgb = round(accuracy_score(y_test,y_pred),3) recall_xgb = round(recall_score(y_test,y_pred, average=&#39;weighted&#39;),3) precision_xgb = round(precision_score(y_test,y_pred, average=&#39;weighted&#39;),3) f1_score_xgb = round(f1_score(y_test,y_pred, average=&#39;weighted&#39;),3) #saving results in the list accuracy_best_params.append(accuracy_xgb) recall_best_params.append(recall_xgb) precision_best_params.append(precision_xgb) f1_best_params.append(f1_score_xgb) . precision recall f1-score support 0 0.85 0.87 0.86 4703 1 0.49 0.45 0.47 1297 accuracy 0.78 6000 macro avg 0.67 0.66 0.67 6000 weighted avg 0.77 0.78 0.78 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred) auc = roc_auc_score(y_test, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . dataframe_best_params = pd.DataFrame(list(zip(models, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) dataframe_best_params.head() . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.758 | 0.758 | 0.754 | 0.755 | . 1 KNN | 0.640 | 0.640 | 0.706 | 0.665 | . 2 SVM | 0.783 | 0.783 | 0.717 | 0.697 | . 3 XGBoost | 0.780 | 0.780 | 0.774 | 0.777 | . dataframe.head() . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.651 | 0.651 | 0.718 | 0.675 | . 1 KNN | 0.605 | 0.605 | 0.711 | 0.638 | . 2 SVM | 0.571 | 0.571 | 0.743 | 0.608 | . 3 XGBoost | 0.777 | 0.777 | 0.768 | 0.772 | . It can be seen that using best parameters improved all the scores. . dataframe.to_csv(&#39;Model_Scores_best_params.csv&#39;, index=False) . Dimensionallity Reduction - PCA . X_train_sc = pd.DataFrame(sc.fit_transform(X_train_res),columns = X_train_res.columns) X_test_sc = pd.DataFrame(sc.transform(X_test),columns = X_test.columns) #performing PCA pca = PCA(n_components = 0.9) X_train_pca = pd.DataFrame(pca.fit_transform(X_train_sc)) X_test_pca = pd.DataFrame(pca.transform(X_test_sc)) print(f&#39;Original: {X_train_res.shape}&#39;) print(f&#39;PCA Transformed: {X_train_pca.shape}&#39;) . Original: (37322, 26) PCA Transformed: (37322, 15) . X_train_pca.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . 0 -1.196002 | -1.088930 | -2.101959 | 1.178225 | -0.551625 | 1.961024 | -0.746287 | -0.812112 | 0.176837 | 0.050608 | 0.099674 | -0.029380 | -0.213249 | 0.747215 | -0.117301 | . 1 -2.400473 | 0.853851 | 0.660715 | -1.491268 | 0.989811 | 0.642099 | 0.207690 | -0.392462 | 0.266411 | 0.056423 | -0.018827 | -0.087463 | 0.001090 | -0.702052 | 0.717397 | . 2 -0.509592 | -1.628711 | -0.928496 | -0.500663 | 0.981187 | -1.050940 | -0.292968 | 0.527187 | -0.160746 | -0.048650 | 0.049174 | 0.063011 | 0.137533 | 0.836358 | 0.067452 | . 3 1.405726 | -4.325725 | 0.584983 | -0.096603 | 0.967941 | -0.896177 | -0.034216 | 0.370994 | -0.111266 | -0.010153 | -0.078502 | -0.039570 | 0.039830 | 0.159046 | -0.935368 | . 4 -1.166613 | -1.331269 | -0.214120 | -0.636438 | 1.221545 | -0.733641 | -0.368034 | 0.678383 | -0.230584 | -0.077418 | 0.153918 | 0.095465 | -0.077673 | -0.052270 | -1.019539 | . recall_pca = [] accuracy_pca = [] precision_pca = [] f1_pca = [] . Logistic Regression . lr_pca = lr.fit(X_train_pca, y_train_res) y_pred_pca = lr_pca.predict(X_test_pca) print(classification_report(y_test, y_pred_pca)) accuracy_lrc_pca = round(accuracy_score(y_test,y_pred_pca),3) recall_lr_pca = round(recall_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) precision_lrc_pca = round(precision_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) f1_score_lr_pca = round(f1_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) #saving results in the list accuracy_pca.append(accuracy_lrc_pca) recall_pca.append(recall_lr_pca) precision_pca.append(precision_lrc_pca) f1_pca.append(f1_score_lr_pca) . precision recall f1-score support 0 0.85 0.71 0.77 4703 1 0.34 0.54 0.41 1297 accuracy 0.67 6000 macro avg 0.59 0.62 0.59 6000 weighted avg 0.74 0.67 0.69 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred_pca) auc = roc_auc_score(y_test, y_pred_pca) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . KNN . knc_pca = knc.fit(X_train_pca, y_train_res) y_pred_pca = knc_pca.predict(X_test_pca) print(classification_report(y_test, y_pred_pca)) accuracy_knc_pca = round(accuracy_score(y_test,y_pred_pca),3) recall_knc_pca = round(recall_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) precision_knc_pca = round(precision_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) f1_score_knc_pca = round(f1_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) #saving results in the list accuracy_pca.append(accuracy_knc_pca) recall_pca.append(recall_knc_pca) precision_pca.append(precision_knc_pca) f1_pca.append(f1_score_knc_pca) . precision recall f1-score support 0 0.84 0.83 0.83 4703 1 0.40 0.41 0.40 1297 accuracy 0.74 6000 macro avg 0.62 0.62 0.62 6000 weighted avg 0.74 0.74 0.74 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred_pca) auc = roc_auc_score(y_test, y_pred_pca) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . SVM . svc_pca = svc.fit(X_train_pca, y_train_res) y_pred_pca = svc_pca.predict(X_test_pca) print(classification_report(y_test, y_pred_pca)) accuracy_svc_pca = round(accuracy_score(y_test,y_pred_pca),3) recall_svc_pca = round(recall_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) precision_svc_pca = round(precision_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) f1_score_svc_pca = round(f1_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) #saving results in the list accuracy_pca.append(accuracy_svc_pca) recall_pca.append(recall_svc_pca) precision_pca.append(precision_svc_pca) f1_pca.append(f1_score_svc_pca) . precision recall f1-score support 0 0.84 0.85 0.85 4703 1 0.44 0.41 0.43 1297 accuracy 0.76 6000 macro avg 0.64 0.63 0.64 6000 weighted avg 0.75 0.76 0.76 6000 . fpr, tpr, _ = roc_curve(y_test, y_pred_pca) auc = roc_auc_score(y_test, y_pred_pca) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . XGBoost . xgb_pca = xgb.fit(X_train_pca, y_train_res) y_pred_pca = xgb_pca.predict(X_test_pca) print(classification_report(y_test, y_pred_pca)) accuracy_xgb_pca = round(accuracy_score(y_test,y_pred_pca),3) recall_xgb_pca = round(recall_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) precision_xgb_pca = round(precision_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) f1_score_xgb_pca = round(f1_score(y_test,y_pred_pca, average=&#39;weighted&#39;),3) #saving results in the list accuracy_pca.append(accuracy_xgb_pca) recall_pca.append(recall_xgb_pca) precision_pca.append(precision_xgb_pca) f1_pca.append(f1_score_xgb_pca) . NameError Traceback (most recent call last) c: Users Robin Downloads Credit_Card_Default.ipynb Cell 72 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/Credit_Card_Default.ipynb#ch0000076?line=0&#39;&gt;1&lt;/a&gt; xgb_pca = xgb.fit(X_train_pca, y_train_res) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/Credit_Card_Default.ipynb#ch0000076?line=1&#39;&gt;2&lt;/a&gt; y_pred_pca = xgb_pca.predict(X_test_pca) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/Credit_Card_Default.ipynb#ch0000076?line=2&#39;&gt;3&lt;/a&gt; print(classification_report(y_test, y_pred_pca)) NameError: name &#39;xgb&#39; is not defined . from sklearn.metrics import plot_confusion_matrix, confusion_matrix plot_confusion_matrix(y_pred_pca, X_test, y_test) . NameError Traceback (most recent call last) c: Users Robin Downloads Credit_Card_Default.ipynb Cell 73 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/Credit_Card_Default.ipynb#ch0000094?line=0&#39;&gt;1&lt;/a&gt; from sklearn.metrics import plot_confusion_matrix, confusion_matrix -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/Credit_Card_Default.ipynb#ch0000094?line=1&#39;&gt;2&lt;/a&gt; plot_confusion_matrix(y_pred_pca, X_test, y_test) NameError: name &#39;y_pred_pca&#39; is not defined . fpr, tpr, _ = roc_curve(y_test, y_pred_pca) auc = roc_auc_score(y_test, y_pred_pca) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() . dataframe_pca = pd.DataFrame(list(zip(models, accuracy_pca, recall_pca, precision_pca, f1_pca)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) . #Without PCA dataframe_best_params . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.758 | 0.758 | 0.754 | 0.755 | . 1 KNN | 0.640 | 0.640 | 0.706 | 0.665 | . 2 SVM | 0.783 | 0.783 | 0.717 | 0.697 | . 3 XGBoost | 0.780 | 0.780 | 0.774 | 0.777 | . dataframe_pca . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.671 | 0.671 | 0.737 | 0.694 | . 1 KNN | 0.739 | 0.739 | 0.741 | 0.740 | . 2 SVM | 0.760 | 0.760 | 0.754 | 0.757 | . 3 XGBoost | 0.755 | 0.755 | 0.765 | 0.759 | . Reducing the number of dimensions, improved the scores for KNN Model but decreased the scores for Logistic, SVM and XGBoost. . XGBoost without reducing dimensions is giving better results as compared to other models, with an accuracy and recall score of around 80%. . dataframe.to_csv(&#39;Model_Scores_pca.csv&#39;, index=False) . Cross Validation . xgb_accuracy_scores = cross_val_score(xgb, X_train_res, y_train_res, cv=5, scoring=&#39;accuracy&#39;) print(&quot;After applying 5-fold cross validation, the different accuracy scores obtained are&quot;) print(&quot; n&quot;) print(&quot;XGBoost: &quot;,list(xgb_accuracy_scores)) print(&quot; n&quot;) print(&quot;Average XGBoost Score&quot;, np.mean(xgb_accuracy_scores)) . After applying 5-fold cross validation, the different accuracy scores obtained are XGBoost: [0.6278633623576692, 0.802277294038848, 0.8869239013933548, 0.8839764201500536, 0.8873258306538049] Average XGBoost Score 0.817673361718746 .",
            "url": "https://apayal.github.io/capstone_project/2022/08/04/Credit_Card_Default_modelling.html",
            "relUrl": "/2022/08/04/Credit_Card_Default_modelling.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://apayal.github.io/capstone_project/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://apayal.github.io/capstone_project/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://apayal.github.io/capstone_project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://apayal.github.io/capstone_project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}