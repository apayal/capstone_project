{
  
    
        "post0": {
            "title": "Predicting credit card defaults using ML",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm from imblearn.over_sampling import SMOTE #Model Building Libraries from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from xgboost import XGBClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.metrics import accuracy_score,classification_report, recall_score, precision_score, f1_score, roc_auc_score, roc_curve from sklearn.metrics import confusion_matrix,plot_confusion_matrix #Dimensionality Reduction from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler sc = StandardScaler() #Cross-Validation from sklearn.model_selection import cross_val_score #settings pd.set_option(&#39;display.max_columns&#39;, None) pd.set_option(&#39;display.max_rows&#39;, None) import warnings warnings.filterwarnings(&quot;ignore&quot;) . c: Users Robin anaconda3 lib site-packages xgboost compat.py:93: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. from pandas import MultiIndex, Int64Index . data = pd.read_csv(&quot;credit-df-dataset-cleaned.csv&quot;) data.head() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 3 | 3 | 0 | 0 | -1 | -1 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | 0 | 3 | 1 | 1 | 1 | 3 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 1 | 1 | 1 | 1 | 1 | 1 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 1 | 1 | 1 | 1 | 1 | 1 | 46990 | 48233 | 49291 | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | 0 | 1 | 0 | 1 | 1 | 1 | 8617 | 5670 | 35835 | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . Y = data[:][&#39;DEFAULT&#39;] X = data.drop([&#39;DEFAULT&#39;,&#39;ID&#39;], axis=1) . X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0) # describes info about train and test set print(&quot;Number transactions X_train dataset: &quot;, X_train.shape) print(&quot;Number transactions y_train dataset: &quot;, y_train.shape) print(&quot;Number transactions X_test dataset: &quot;, X_test.shape) print(&quot;Number transactions y_test dataset: &quot;, y_test.shape) . Number transactions X_train dataset: (24000, 26) Number transactions y_train dataset: (24000,) Number transactions X_test dataset: (6000, 26) Number transactions y_test dataset: (6000,) . Balance the Imbalanced dataset . As you can see below the dataset is quite imbalanced. So in order to avoid bias to the majority class we will use SMOTE to oversample the 1 value and hopefully give less biased prediction results. . data.DEFAULT.value_counts(normalize=True)*100 . 0 77.88 1 22.12 Name: DEFAULT, dtype: float64 . print(&quot;Before OverSampling, counts of label &#39;1&#39;: {}&quot;.format(sum(y_train == 1))) print(&quot;Before OverSampling, counts of label &#39;0&#39;: {} n&quot;.format(sum(y_train == 0))) sm = SMOTE(random_state = 2) X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel()) print(&#39;After OverSampling, the shape of train_X: {}&#39;.format(X_train_res.shape)) print(&#39;After OverSampling, the shape of train_y: {} n&#39;.format(y_train_res.shape)) print(&quot;After OverSampling, counts of label &#39;1&#39;: {}&quot;.format(sum(y_train_res == 1))) print(&quot;After OverSampling, counts of label &#39;0&#39;: {}&quot;.format(sum(y_train_res == 0))) . Before OverSampling, counts of label &#39;1&#39;: 5339 Before OverSampling, counts of label &#39;0&#39;: 18661 After OverSampling, the shape of train_X: (37322, 26) After OverSampling, the shape of train_y: (37322,) After OverSampling, counts of label &#39;1&#39;: 18661 After OverSampling, counts of label &#39;0&#39;: 18661 . X_train_res.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 20000 | 1 | 44 | 1 | 1 | 3 | 1 | 1 | -1 | 17095 | 19112 | 17980 | 18780 | 0 | 0 | 3000 | 0 | 1000 | 1000 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 260000 | 1 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 165 | 274 | 165 | 333 | 165 | 165 | 274 | 165 | 333 | 165 | 293 | 0 | 1 | 0 | 1 | 0 | . 2 20000 | 0 | 39 | 3 | 1 | 1 | 1 | 1 | 1 | 20264 | 20328 | 19299 | 19928 | 20204 | 20398 | 1500 | 1500 | 900 | 700 | 1480 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 30000 | 0 | 23 | 3 | 3 | 3 | 3 | 3 | 3 | 28224 | 29276 | 28635 | 30127 | 30525 | 29793 | 1800 | 150 | 2250 | 1000 | 0 | 700 | 0 | 1 | 0 | 0 | 0 | . 4 10000 | 0 | 29 | 1 | 1 | 1 | 1 | 1 | 1 | 8275 | 8409 | 8600 | 9470 | 6690 | 9690 | 2800 | 2000 | 1500 | 900 | 3000 | 0 | 0 | 1 | 0 | 0 | 0 | . print(&quot;Number transactions X_train dataset: &quot;, X_train_res.shape) print(&quot;Number transactions y_train dataset: &quot;, y_train_res.shape) print(&quot;Number transactions X_test dataset: &quot;, X_test.shape) print(&quot;Number transactions y_test dataset: &quot;, y_test.shape) . Number transactions X_train dataset: (37322, 26) Number transactions y_train dataset: (37322,) Number transactions X_test dataset: (6000, 26) Number transactions y_test dataset: (6000,) . Initial Models . lr=LogisticRegression() knc = KNeighborsClassifier() svc = SVC() dtc = DecisionTreeClassifier() xgb = XGBClassifier() adb = AdaBoostClassifier() #to store results models = [&#39;Logistic Regression&#39;, &quot;KNN&quot;, &quot;SVM&quot;,&quot;Decision Tree&quot;, &quot;XGBoost&quot;, &quot;AdaBoost&quot;] recall = [] accuracy = [] precision = [] f1 = [] . def run_model(model, train_x, train_y, test_x, test_y, store_acc, store_recall, store_prec, store_f1): model = model.fit(train_x, train_y) y_pred = model.predict(test_x) print(classification_report(test_y, y_pred)) #plotting ROC curve fpr, tpr, _ = roc_curve(test_y, y_pred) auc = roc_auc_score(test_y, y_pred) #create ROC curve plt.plot(fpr,tpr,label=&quot;AUC=&quot;+str(auc)) plt.ylabel(&#39;True Positive Rate&#39;) plt.xlabel(&#39;False Positive Rate&#39;) plt.legend(loc=4) plt.show() accuracy_ = round(accuracy_score(test_y, y_pred),3) recall_ = round(recall_score(test_y, y_pred, average=&#39;weighted&#39;),3) precision_ = round(precision_score(test_y, y_pred, average=&#39;weighted&#39;),3) f1_score_ = round(f1_score(test_y, y_pred, average=&#39;weighted&#39;),3) #saving results in the list store_recall.append(recall_) store_acc.append(accuracy_) store_prec.append(precision_) store_f1.append(f1_score_) . ROC and AUC curve This is an important metric to consider when we are concerned about the negative class but is more often optimized through AUC. AUC is the area under the ROC curve (AUC). It is the curve formed from plotting True Positive Rate and False Positive Rate at various probability thresholds. It ranges from 0 to 1, with an uninformative classifier being 0.5 and a perfect classifier at 1. We will be plotting it after running each model to help get a sense of the usefullness of the model. . Logistic Regression . run_model(lr, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.83 0.71 0.76 4703 1 0.31 0.48 0.38 1297 accuracy 0.66 6000 macro avg 0.57 0.59 0.57 6000 weighted avg 0.72 0.66 0.68 6000 . It can be seen in the Classification report that the recall score for label 0 is 0.76 and for label 1 is 0.38. . The weighted recall score for both the labels came out to be 0.65. The AUC is 0.59 which is not that imformative. . Log Transformation . From the previous notebook we know there some numerical variables that are skewed in nature. Log Transformation is one of the technique that can help in removing the skewness from data by making the data Normally Distributed. . Numerical Variables: &#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1-6&#39; and &#39;PAY_AMT1-6&#39;are skewed in the nature. . X_train_log = X_train_res.copy() X_train_log.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 20000 | 1 | 44 | 1 | 1 | 3 | 1 | 1 | -1 | 17095 | 19112 | 17980 | 18780 | 0 | 0 | 3000 | 0 | 1000 | 1000 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 260000 | 1 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 165 | 165 | 274 | 165 | 333 | 165 | 165 | 274 | 165 | 333 | 165 | 293 | 0 | 1 | 0 | 1 | 0 | . 2 20000 | 0 | 39 | 3 | 1 | 1 | 1 | 1 | 1 | 20264 | 20328 | 19299 | 19928 | 20204 | 20398 | 1500 | 1500 | 900 | 700 | 1480 | 0 | 0 | 1 | 0 | 0 | 0 | . 3 30000 | 0 | 23 | 3 | 3 | 3 | 3 | 3 | 3 | 28224 | 29276 | 28635 | 30127 | 30525 | 29793 | 1800 | 150 | 2250 | 1000 | 0 | 700 | 0 | 1 | 0 | 0 | 0 | . 4 10000 | 0 | 29 | 1 | 1 | 1 | 1 | 1 | 1 | 8275 | 8409 | 8600 | 9470 | 6690 | 9690 | 2800 | 2000 | 1500 | 900 | 3000 | 0 | 0 | 1 | 0 | 0 | 0 | . From the previous notebook we know there is not much skewness in Pay columns so we will be focusing on [&#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1&#39;,&#39;BILL_AMT2&#39;,&#39;BILL_AMT3&#39;,&#39;BILL_AMT4&#39;,&#39;BILL_AMT5&#39;,&#39;BILL_AMT6&#39;&#39;PAY_AMT1&#39;,&#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;,&#39;PAY_AMT4&#39;,&#39;PAY_AMT5&#39;,&#39;PAY_AMT6&#39;] columns only as they have power law distribution where there is a big 0 and long right tail. . Log Transformation of negative values and 0 is undefined so first I will be converting all negative values to positive by adding a constant in all the values. . constant = 400000 col = [&#39;LIMIT_BAL&#39;,&#39;AGE&#39;,&#39;BILL_AMT1&#39;,&#39;BILL_AMT2&#39;,&#39;BILL_AMT3&#39;,&#39;BILL_AMT4&#39;,&#39;BILL_AMT5&#39;,&#39;BILL_AMT6&#39;,&#39;PAY_AMT1&#39;,&#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;,&#39;PAY_AMT4&#39;,&#39;PAY_AMT5&#39;,&#39;PAY_AMT6&#39;] for columns in col: X_train_log[columns] = X_train_log[columns]+constant X_train_log[columns] = np.log10(X_train_log[columns]) . X_train_log.head() . LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 5.623249 | 1 | 5.602108 | 1 | 1 | 3 | 1 | 1 | -1 | 5.620235 | 5.622330 | 5.621156 | 5.621986 | 5.602060 | 5.602060 | 5.605305 | 5.602060 | 5.603144 | 5.603144 | 5.602060 | 5.602060 | 0 | 0 | 1 | 0 | 0 | . 1 5.819544 | 1 | 5.602093 | 0 | 0 | 0 | 0 | 0 | 0 | 5.602239 | 5.602239 | 5.602357 | 5.602239 | 5.602421 | 5.602239 | 5.602239 | 5.602357 | 5.602239 | 5.602421 | 5.602239 | 5.602378 | 0 | 1 | 0 | 1 | 0 | . 2 5.623249 | 0 | 5.602102 | 3 | 1 | 1 | 1 | 1 | 1 | 5.623522 | 5.623588 | 5.622524 | 5.623175 | 5.623460 | 5.623661 | 5.603686 | 5.603686 | 5.603036 | 5.602819 | 5.603664 | 5.602060 | 0 | 1 | 0 | 0 | 0 | . 3 5.633468 | 0 | 5.602085 | 3 | 3 | 3 | 3 | 3 | 3 | 5.631671 | 5.632737 | 5.632088 | 5.633597 | 5.633998 | 5.633259 | 5.604010 | 5.602223 | 5.604496 | 5.603144 | 5.602060 | 5.602819 | 0 | 1 | 0 | 0 | 0 | . 4 5.612784 | 0 | 5.602091 | 1 | 1 | 1 | 1 | 1 | 1 | 5.610953 | 5.611095 | 5.611298 | 5.612222 | 5.609263 | 5.612455 | 5.605089 | 5.604226 | 5.603686 | 5.603036 | 5.605305 | 5.602060 | 0 | 1 | 0 | 0 | 0 | . #cannot use run_model function as we the syntax would be different (not storing the results in the dataframe) lr = lr.fit(X_train_log, y_train_res) y_pred = lr.predict(X_test) print(classification_report(y_test, y_pred)) . precision recall f1-score support 0 0.00 0.00 0.00 4703 1 0.22 1.00 0.36 1297 accuracy 0.22 6000 macro avg 0.11 0.50 0.18 6000 weighted avg 0.05 0.22 0.08 6000 . Transforming into log is giving recall value for label 1 as 0.00, which means it is predicting all the classes as 0. It is performing poorly so we will be avoiding log transformation in further steps. . KNN . run_model(knc, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.83 0.62 0.71 4703 1 0.28 0.54 0.37 1297 accuracy 0.60 6000 macro avg 0.56 0.58 0.54 6000 weighted avg 0.71 0.60 0.64 6000 . Although AUC is lower the recall score for default is higher which is promising. . SVM . run_model(svc, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.87 0.54 0.66 4703 1 0.29 0.70 0.41 1297 accuracy 0.57 6000 macro avg 0.58 0.62 0.54 6000 weighted avg 0.74 0.57 0.61 6000 . Decision Tree . run_model(dtc, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.83 0.76 0.79 4703 1 0.34 0.44 0.38 1297 accuracy 0.69 6000 macro avg 0.58 0.60 0.59 6000 weighted avg 0.73 0.69 0.71 6000 . XGBoost . run_model(xgb, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.85 0.87 0.86 4703 1 0.48 0.43 0.45 1297 accuracy 0.78 6000 macro avg 0.67 0.65 0.66 6000 weighted avg 0.77 0.78 0.77 6000 . AdaBoost . run_model(adb, X_train_res, y_train_res, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.86 0.83 0.84 4703 1 0.45 0.49 0.47 1297 accuracy 0.76 6000 macro avg 0.65 0.66 0.66 6000 weighted avg 0.77 0.76 0.76 6000 . So far XGBoost and Adaboost have the best scores in both AUC and recall score. Howevwer we can definetly get higher scores if can find the optimum parameters for the models. . dataframe = pd.DataFrame(list(zip(models, accuracy, recall, precision, f1)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) dataframe . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.656 | 0.656 | 0.718 | 0.679 | . 1 KNN | 0.605 | 0.605 | 0.711 | 0.638 | . 2 SVM | 0.571 | 0.571 | 0.743 | 0.608 | . 3 Decision Tree | 0.691 | 0.691 | 0.725 | 0.705 | . 4 XGBoost | 0.777 | 0.777 | 0.768 | 0.772 | . 5 AdaBoost | 0.758 | 0.758 | 0.767 | 0.762 | . Hyperameter tuning with GridSearchCV . logistic_parameters = {&#39;solver&#39;:(&#39;sag&#39;, &#39;saga&#39;,&#39;newton-cg&#39;,&#39;liblinear&#39;,&#39;lbfgs&#39;),&quot;penalty&quot;:(&quot;l1&quot;,&quot;l2&quot;,&quot;None&quot;)} knn_parameters = {&#39;n_neighbors&#39;:(range(5,10)), &quot;algorithm&quot;:(&quot;auto&quot;,&quot;kd_tree&quot;)} svm_parameters = {&quot;gamma&quot;:(1, 0.1, 0.01),&quot;kernel&quot;:(&quot;linear&quot;,&quot;rbf&quot;)} dtc_parameters = {&#39;criterion&#39;:(&quot;gini&quot;,&quot;entropy&quot;,&quot;log_loss&quot;), &#39;max_depth&#39;:(range(5,15))} xgboost_parameters = {&quot;n_estimators&quot;: (45, 50, 55)} adb_parameters = {&#39;n_estimators&#39;:(45, 50, 55), &quot;learning_rate&quot;:(0.01, 0.1, 1)} . parameter_dictionary = {} #function to find best parameters for each model using GridSearchCV def optimize_params(model, parameter_list): #taking only a sub-sample of training data so to reduce the runtime. X_sub = X_train_res[0:10000] y_sub = y_train_res[0:10000] grid = GridSearchCV(model, parameter_list, cv=3) grid.fit(X_sub, y_sub) parameter_dictionary[model] = grid.best_params_ . Now we can call the function optimize_params to get best parameters for the model by using GridSearchCV. Note: SVM and XGB can take a lot of time even after giving just one parameter. In the below models, I have used the best parameter to train the model. . # optimize_params(lr, logistic_parameters) # optimize_params(knc, knn_parameters) # optimize_params(svc, svm_parameters) # optimize_params(dtc, dtc_parameters) #optimize_params(xgb, xgboost_parameters) # optimize_params(adb, adb_parameters) . . {} . lr=LogisticRegression(solver=&#39;liblinear&#39;, penalty=&#39;l1&#39;) knc = KNeighborsClassifier(n_neighbors = 6, algorithm=&#39;kd_tree&#39;) svc = SVC(kernel=&quot;rbf&quot;, gamma=1) dtc = DecisionTreeClassifier(criterion=&#39;gini&#39;, max_depth=5) xgb = XGBClassifier(n_estimators = 50) adb = AdaBoostClassifier(n_estimators=55, learning_rate=1) . recall_best_params = [] accuracy_best_params = [] precision_best_params = [] f1_best_params = [] . Retrain models based on best parameters . Logistic Regression . run_model(lr, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.84 0.85 0.85 4703 1 0.44 0.42 0.43 1297 accuracy 0.76 6000 macro avg 0.64 0.63 0.64 6000 weighted avg 0.75 0.76 0.76 6000 . KNN . run_model(knc, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.82 0.69 0.75 4703 1 0.29 0.46 0.35 1297 accuracy 0.64 6000 macro avg 0.56 0.57 0.55 6000 weighted avg 0.71 0.64 0.66 6000 . SVM . run_model(svc, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.79 0.99 0.88 4703 1 0.47 0.02 0.04 1297 accuracy 0.78 6000 macro avg 0.63 0.51 0.46 6000 weighted avg 0.72 0.78 0.70 6000 . Decision Tree . run_model(dtc, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.87 0.74 0.80 4703 1 0.38 0.58 0.46 1297 accuracy 0.71 6000 macro avg 0.63 0.66 0.63 6000 weighted avg 0.76 0.71 0.73 6000 . XGBoost . run_model(xgb, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.85 0.87 0.86 4703 1 0.49 0.45 0.47 1297 accuracy 0.78 6000 macro avg 0.67 0.66 0.67 6000 weighted avg 0.77 0.78 0.78 6000 . AdaBoost . run_model(adb, X_train_res, y_train_res, X_test, y_test, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params) . precision recall f1-score support 0 0.86 0.83 0.84 4703 1 0.45 0.49 0.47 1297 accuracy 0.76 6000 macro avg 0.65 0.66 0.66 6000 weighted avg 0.77 0.76 0.76 6000 . dataframe_best_params = pd.DataFrame(list(zip(models, accuracy_best_params, recall_best_params, precision_best_params, f1_best_params)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) dataframe_best_params . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.758 | 0.758 | 0.754 | 0.755 | . 1 KNN | 0.640 | 0.640 | 0.706 | 0.665 | . 2 SVM | 0.783 | 0.783 | 0.717 | 0.697 | . 3 Decision Tree | 0.708 | 0.708 | 0.762 | 0.727 | . 4 XGBoost | 0.780 | 0.780 | 0.774 | 0.777 | . 5 AdaBoost | 0.759 | 0.759 | 0.767 | 0.763 | . dataframe # original model run . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.656 | 0.656 | 0.718 | 0.679 | . 1 KNN | 0.605 | 0.605 | 0.711 | 0.638 | . 2 SVM | 0.571 | 0.571 | 0.743 | 0.608 | . 3 Decision Tree | 0.691 | 0.691 | 0.725 | 0.705 | . 4 XGBoost | 0.777 | 0.777 | 0.768 | 0.772 | . 5 AdaBoost | 0.758 | 0.758 | 0.767 | 0.762 | . It can be seen that using best parameters improved all the scores, in particular logistic regression, KNN and SVM had the most significant increase. However although SVM has the highest recall now in the classification report we saw that for the test set the recall was only 0.2! XGBoost and ADAboost are more rounded instead and we should focus more on them. . Feature importance . feature_important = xgb.get_booster().get_score(importance_type=&#39;weight&#39;) keys = list(feature_important.keys()) values = list(feature_important.values()) data = pd.DataFrame(data=values, index=keys, columns=[&quot;score&quot;]).sort_values(by = &quot;score&quot;, ascending=False) data = data.nlargest(50, columns=&quot;score&quot;).reset_index().rename(columns = {&#39;index&#39;:&#39;Features&#39;}) plt.figure(figsize=(14,8)) plt.barh(data[&#39;Features&#39;], data[&#39;score&#39;], color=&#39;coral&#39;) plt.ylabel(&#39;Features&#39;) plt.xlabel(&#39;Scores&#39;) plt.title(&quot;Feature Importance Chart&quot;) plt.show() . dataframe.to_csv(&#39;Model_Scores_best_params.csv&#39;, index=False) . The feature importance graph above shows us that at the moment with the current model finantial data is the most important in prediction. Bill amount limit balance and repayment amounts are top most important features and age is the highest demographic feature in predicting default rates. An ideal model would have the demographic data as well as some financial data as the key to predicting defaults. That we we could predict with just background information and smalller credit history. Before doing overall PCA to reduce dimensions I want to test to see if we only take the top 5 most important features + highest ranked demographic feature are we able to get a fairly accurate score. . c=[&#39;BILL_AMT1&#39;,&#39;LIMIT_BAL&#39;,&#39;PAY_AMT1&#39;,&#39;PAY_AMT6&#39;,&#39;PAY_AMT4&#39;,&#39;AGE&#39;] Y1 = data[&#39;DEFAULT&#39;] X1 = data[c] X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size = 0.2, random_state = 0) run_model(xgb, X_train, y_train, X_test, y_test, accuracy, recall, precision, f1) . precision recall f1-score support 0 0.80 0.97 0.88 4703 1 0.52 0.13 0.20 1297 accuracy 0.79 6000 macro avg 0.66 0.55 0.54 6000 weighted avg 0.74 0.79 0.73 6000 . Unfortunelty with the current model it is not very good at calculating the test set. Although ideally we would&#39;ve liked to have a higher rate of false negatives being detected the model is not able to do so with so little information. . Dimensionallity Reduction - PCA . X_train_sc = pd.DataFrame(sc.fit_transform(X_train_res),columns = X_train_res.columns) X_test_sc = pd.DataFrame(sc.transform(X_test),columns = X_test.columns) #performing PCA pca = PCA(n_components = 0.9) X_train_pca = pd.DataFrame(pca.fit_transform(X_train_sc)) X_test_pca = pd.DataFrame(pca.transform(X_test_sc)) print(f&#39;Original: {X_train_res.shape}&#39;) print(f&#39;PCA Transformed: {X_train_pca.shape}&#39;) . Original: (37322, 26) PCA Transformed: (37322, 15) . X_train_pca.head() . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . 0 -1.196002 | -1.088930 | -2.101959 | 1.178225 | -0.551625 | 1.961024 | -0.746287 | -0.812112 | 0.176837 | 0.050608 | 0.099674 | -0.029380 | -0.213249 | 0.747215 | -0.117301 | . 1 -2.400473 | 0.853851 | 0.660715 | -1.491268 | 0.989811 | 0.642099 | 0.207690 | -0.392462 | 0.266411 | 0.056423 | -0.018827 | -0.087463 | 0.001090 | -0.702052 | 0.717397 | . 2 -0.509592 | -1.628711 | -0.928496 | -0.500663 | 0.981187 | -1.050940 | -0.292968 | 0.527187 | -0.160746 | -0.048650 | 0.049174 | 0.063011 | 0.137533 | 0.836358 | 0.067452 | . 3 1.405726 | -4.325725 | 0.584983 | -0.096603 | 0.967941 | -0.896177 | -0.034216 | 0.370994 | -0.111266 | -0.010153 | -0.078502 | -0.039570 | 0.039830 | 0.159046 | -0.935368 | . 4 -1.166613 | -1.331269 | -0.214120 | -0.636438 | 1.221545 | -0.733641 | -0.368034 | 0.678383 | -0.230584 | -0.077418 | 0.153918 | 0.095465 | -0.077673 | -0.052270 | -1.019539 | . recall_pca = [] accuracy_pca = [] precision_pca = [] f1_pca = [] . Rerun models on PCA data . Logistic Regression . run_model(lr, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.85 0.71 0.77 4703 1 0.34 0.54 0.41 1297 accuracy 0.67 6000 macro avg 0.59 0.62 0.59 6000 weighted avg 0.74 0.67 0.69 6000 . KNN . run_model(knc, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.84 0.83 0.83 4703 1 0.40 0.41 0.40 1297 accuracy 0.74 6000 macro avg 0.62 0.62 0.62 6000 weighted avg 0.74 0.74 0.74 6000 . SVM . run_model(svc, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.84 0.85 0.85 4703 1 0.44 0.41 0.43 1297 accuracy 0.76 6000 macro avg 0.64 0.63 0.64 6000 weighted avg 0.75 0.76 0.76 6000 . Decision tree . run_model(dtc, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.85 0.80 0.82 4703 1 0.40 0.49 0.44 1297 accuracy 0.73 6000 macro avg 0.62 0.64 0.63 6000 weighted avg 0.75 0.73 0.74 6000 . XGBoost . run_model(xgb, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.85 0.83 0.84 4703 1 0.44 0.49 0.46 1297 accuracy 0.76 6000 macro avg 0.65 0.66 0.65 6000 weighted avg 0.76 0.76 0.76 6000 . AdaBoost . run_model(adb, X_train_pca, y_train_res, X_test_pca, y_test, accuracy_pca, recall_pca, precision_pca, f1_pca) . precision recall f1-score support 0 0.85 0.78 0.82 4703 1 0.39 0.49 0.43 1297 accuracy 0.72 6000 macro avg 0.62 0.64 0.62 6000 weighted avg 0.75 0.72 0.73 6000 . Model evaluation . dataframe_pca = pd.DataFrame(list(zip(models, accuracy_pca, recall_pca, precision_pca, f1_pca)), columns = [&#39;Models&#39;,&#39;Accuracy&#39;,&#39;Recall&#39;, &#39;Precision&#39;,&#39;f1-score&#39;]) . #Without PCA dataframe_best_params . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.758 | 0.758 | 0.754 | 0.755 | . 1 KNN | 0.640 | 0.640 | 0.706 | 0.665 | . 2 SVM | 0.783 | 0.783 | 0.717 | 0.697 | . 3 Decision Tree | 0.708 | 0.708 | 0.762 | 0.727 | . 4 XGBoost | 0.780 | 0.780 | 0.774 | 0.777 | . 5 AdaBoost | 0.759 | 0.759 | 0.767 | 0.763 | . dataframe_pca . Models Accuracy Recall Precision f1-score . 0 Logistic Regression | 0.671 | 0.671 | 0.737 | 0.694 | . 1 KNN | 0.739 | 0.739 | 0.741 | 0.740 | . 2 SVM | 0.760 | 0.760 | 0.754 | 0.757 | . 3 Decision Tree | 0.730 | 0.730 | 0.753 | 0.740 | . 4 XGBoost | 0.755 | 0.755 | 0.765 | 0.759 | . 5 AdaBoost | 0.722 | 0.722 | 0.749 | 0.733 | . Reducing the number of dimensions, improved the scores for KNN and decision tree Model but decreased the scores for rest of the models . XGBoost without reducing dimensions is giving better results as compared to other models, with an accuracy and recall score of around 80%. . dataframe.to_csv(&#39;Model_Scores_pca.csv&#39;, index=False) . Cross Validation . xgb_accuracy_scores = cross_val_score(xgb, X_train_res, y_train_res, cv=5, scoring=&#39;accuracy&#39;) print(&quot;After applying 5-fold cross validation, the different accuracy scores obtained are&quot;) print(&quot; n&quot;) print(&quot;XGBoost: &quot;,list(xgb_accuracy_scores)) print(&quot; n&quot;) print(&quot;Average XGBoost Score&quot;, np.mean(xgb_accuracy_scores)) . After applying 5-fold cross validation, the different accuracy scores obtained are XGBoost: [0.6278633623576692, 0.802277294038848, 0.8869239013933548, 0.8839764201500536, 0.8873258306538049] Average XGBoost Score 0.817673361718746 . #plotting confusion matrix on best performing model X_train_new, X_Val, Y_train_new, Y_Val = train_test_split(X_train, y_train, test_size=0.30) xgb1 = XGBClassifier(n_estimators = 50) xgb1.fit(X_train_new,Y_train_new) y_pred = xgb1.predict(X_test) plot_confusion_matrix(xgb1, X_test, y_test) . precision recall f1-score support 0 0.85 0.95 0.89 4703 1 0.65 0.37 0.48 1297 accuracy 0.82 6000 macro avg 0.75 0.66 0.68 6000 weighted avg 0.80 0.82 0.80 6000 . Conclusions . As we can see even with the current best model XGBoost we are still getting about 812/6000 incorrectly identified false negatives. Although this is about 13.5% we ideally want a recall percentage closer to only 5% error rate. . However we were able to significantly increase accuracy and recall rates throuh various means mostly oversampling to balance dataset, hyperparameter tuning and pca for some models. For models like Decision trees or KNN which are more prone to overfitting pruning correlated features can help improve accuracy as it has done for us. Although in theory it should improve other models since it reduces features that are correlated but those features are all high up in importance scale. As we saw that though pay_amt columns are have a high correlation with each other they are some of the most important features in prediction. So reducing the dimensions actually gives worse results to models such as XGBoost. .",
            "url": "https://apayal.github.io/capstone_project/2022/08/07/Modelling.html",
            "relUrl": "/2022/08/07/Modelling.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Predicting credit card defaults",
            "content": "Notebook 2 - EDA . Exploring the data . Table of Contents . Exploratory Data Analysis 1.1 Overview of Data 1.2 Analyse relationship between features 1.3 Summary | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data credit_df_dataset_cleaned.csv&#39;) . . Overview of the data . credit_df.head() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 3 | 3 | 0 | 0 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | 0 | 3 | 1 | 1 | 1 | 3 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | 0 | 1 | 0 | 1 | 1 | 1 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 28 columns . credit_df.describe() . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 15000.500000 | 167484.322667 | 0.603733 | 35.485500 | 0.983300 | 0.866233 | 0.833800 | 0.779333 | 0.733800 | 0.708900 | ... | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | 0.352833 | 0.467667 | 0.163900 | 0.532133 | 0.012567 | . std 8660.398374 | 129747.661567 | 0.489129 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | 1.133187 | 1.149988 | ... | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | 0.477859 | 0.498962 | 0.370191 | 0.498975 | 0.111396 | . min 1.000000 | 10000.000000 | 0.000000 | 21.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 7500.750000 | 50000.000000 | 0.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 15000.500000 | 140000.000000 | 1.000000 | 34.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | ... | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | . 75% 22500.250000 | 240000.000000 | 1.000000 | 41.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | ... | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | . max 30000.000000 | 1000000.000000 | 1.000000 | 79.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | 9.000000 | ... | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 8 rows × 28 columns . plt.figure(figsize=(10,6)) sns.boxplot( x=&#39;Marriage_Single&#39;, y=&#39;AGE&#39;,hue=&#39;SEX&#39;, data=credit_df, palette=&#39;pastel&#39;) plt.show() . Initial observations . Slightly larger number of female&#39;s than male. | Majority of the people are either married or single | Average age of the customers is 35 years old. | Married customers are in thier mid 30&#39;s to 40&#39;s and single customers are in thier mid 20&#39;s to 30&#39;s and women are slightly younger in both sets. | Most of the customers are well educated i.e university undergraduates | Amount of limit balance varies quite a bit as the standard deviation is quite large. | In general people do not default on thier payments, however we want to focus on those who do default | . . Analyse relationship between variables . Correlation . corr = credit_df.corr(method=&#39;spearman&#39;) mask = np.zeros_like(corr,dtype=bool) mask[np.triu_indices_from(mask)]=True cmap = sns.diverging_palette(230, 20, as_cmap=True) with sns.axes_style(&quot;white&quot;): plt.subplots(figsize=(18, 18)) sns.heatmap(corr, mask=mask, square=True, linewidths=.3, fmt=&#39;.2f&#39;, cmap=cmap, annot=True, annot_kws={&quot;size&quot;: 12}) plt.title(&#39;Correlation matrix&#39;, size=15) plt.show() #ref - https://seaborn.pydata.org/examples/many_pairwise_correlations.html . The column that we are most interested in is the default column. Notably with the default column, Pay_1 has the highest correlation followed by the rest of the PAY columns. Limit bal is negatively correlated but has second highest correlation coefficient after the PAY columns. . There are also many correlated columns especially the PAY columns are all highly correlated with each other, as are the BILL_AMT and PAY_AMT. It is likely I will have to perform PCA on this dataset to get rid of multicolinearity. However it will not be needed for all of the different types of models e.g. XGBoost or Decision trees. . Target column - Default . Initial expectations . Looking at the target column we know it only has 2 values - default on payment next month- Yes or no. I am contructing a pie chart to see the distribution of the column. I expect the data to be leaning towards more non-defaulters i.e 0 values. As people do tend to pay thier credit card bills on time usually to avoid late fee&#39;s encurring or debt which can be exorbitant depending on company policy (unless there is a major economic event like recession). . However it is hard to say what percentage of a split we will see as it could be 60:40, 55:45, 70:30 etc. . target = credit_df[&#39;DEFAULT&#39;] . colours = sns.color_palette(&#39;Set2&#39;)[0:5] (credit_df[&#39;DEFAULT&#39;].value_counts(normalize=True)*100).plot(kind=&#39;pie&#39;,autopct=&#39;%.0f%%&#39;,colors=colours,legend=True, figsize=(14,6),startangle=75) # plot using value counts and normalise as it shows us the distribution as a percentage of the whole column plt.legend([&#39;0:Not default&#39;,&#39;1: Default&#39;],loc=&#39;lower right&#39;) plt.title(&#39;Default on payment next month?&#39;) plt.show() . As we know from before that this is a very imbalanced dataset. This is normal as most people will have paid thier credit card bill on time to build good credit history and avoid fees. However I will have to do upsampling on the smaller class to avoid bias to the majority class, which can cause the number of false negatives predicted to be higher. . Univariate analysis . The ID column is just an identifier, which also has no predictive value this is something we can remove later when modelling. . Column: LIMIT_BAL . #finding the top values in the column (credit_df[&#39;LIMIT_BAL&#39;].value_counts(normalize=True)*100).head(5) . 50000 11.216667 20000 6.586667 30000 5.366667 80000 5.223333 200000 5.093333 Name: LIMIT_BAL, dtype: float64 . credit_df[&#39;LIMIT_BAL&#39;].describe() . count 30000.000000 mean 167484.322667 std 129747.661567 min 10000.000000 25% 50000.000000 50% 140000.000000 75% 240000.000000 max 1000000.000000 Name: LIMIT_BAL, dtype: float64 . The most common credit balance limits are 50k, 20k and 30k respectively. Compared to the minimum 10k and maximum of 1000k, the common figures are on the lower end of the range. However the maximum amount is likely to be an outlier as its more than 2 standard deviations away from the mean. . (credit_df[&#39;LIMIT_BAL&#39;].mean())+(2*credit_df[&#39;LIMIT_BAL&#39;].std()) . 426979.64580105676 . We might need to remove any outliers from this column to avoid getting skewed results. Let&#39;s check the distribution of the limit column first. . limit_log=np.log(credit_df[&#39;LIMIT_BAL&#39;]) #convert the column into log-scale #subplot to compare the difference in distribution plt.subplots(1, 2, figsize = (9, 5)) plt.subplot(1, 2, 1) sns.histplot(data=credit_df[&#39;LIMIT_BAL&#39;],bins=50,kde=True) plt.subplot(1, 2, 2) sns.histplot(limit_log, kde=True) plt.tight_layout() plt.show() . The graph on the left shows the initial distribution of the Limit balance column. It seems to have power law distribution where quite skewed with a long right tail and most values frequent near 0. Since the data is not normally distributed this can cause problems for clustering methods we may use as they rely on distances and densities. . On the right is the same feature that has been log transformed. This gives us a more normally distributed graph although not perfect, should help in increasing accuracy of regression and classification models. Some of the other numerical features could also have a similar power law distribution so we can check if those features might benefit from transformation so they can be more normal. . columns = [&#39;AGE&#39;,&#39;PAY_1&#39;,&#39;BILL_AMT1&#39;,&#39;PAY_AMT1&#39;] for col in columns: plt.subplots(1, 2, figsize = (10, 3)) plt.tight_layout() # plot the original features on the left plt.subplot(1, 2, 1) plt.title(f&#39;Original: {col}&#39;) sns.histplot(credit_df[col], bins = 40) # plot the features on a log scale on the right to compare plt.subplot(1, 2, 2) plt.title(f&#39;Log transformed: {col}&#39;) sns.histplot(credit_df[col], bins = 40) plt.yscale(&#39;log&#39;) plt.tight_layout() plt.show() . Similarly Bill_amount and pay_amount also suffer from the same problem as limit balance. If we transform them we get a slighly more normally distributed feature however it still leans towards more values near 0. It does seem to help the pay column become more balanced as with the distribution of age. However how effective it is will only be dtermined by how much of a difference in accuracy of the model we could possibly get. . . Feature analysis against target column . Now we will be doing a bivariate analysis against the other features verses the target column to infer how important they may be to predicting customer defaults as well as how the data is distributed and portion of the values in the columns. Since our target feature is a catergorical column box plots and bar charts are the most suitable for comparison. . # create a function to compare the categorical columns against default column def countplot_comparison(feature1, width): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 5)) s = sns.countplot(ax=ax1, x=target, hue=feature1, data=credit_df, palette=&#39;Set2&#39;) plt.title(f&#39;Default On Payment vs {feature1}&#39;) plt.show(); . Column: SEX . countplot_comparison(&#39;SEX&#39;,6) . As you can see from the bar chart the amount and portion of women who pay thier balance on time is significantly higher than the number of men who do. The portion seems to be roughly at a rate of 2:3 of men to women. Whereas for customers who don&#39;t pay on time the proportion is much closer together. Additionally as we know from our initial observations is there is slighlty more women included in the dataset. From this we can infer that women are more likely to clear thier payments next month and men are slighlty more likely to default on thier payments next month. . Column: Education history . [countplot_comparison(x, 10) for x in credit_df.columns if x.startswith(&#39;E&#39;)] . [None, None, None] . Looking at these particular columns it is a bit harder to comment on the distribution as there are 3 options so for each feature 1 represents the 1 value but 0 represents 2 other values. We can see that people who have been to graduate school (masters, phd etc) have the smallest proportion of people who default compared to people who don&#39;t. It is possible that people who have a higher education are less likely to default however it needs more investigation. . Prportion of defaults in university education and high school is slightly higher compared to higher graduation students. This could be as we know from the background that banks targeted younger students into getting credit cards with initial low interest rates. It&#39;s possible many of these people then overspent and now are struggling to pay it off. . Column: Marital status . [countplot_comparison(x ,10) for x in credit_df.columns if x.startswith(&#39;M&#39;)] . AttributeError Traceback (most recent call last) Input In [109], in &lt;cell line: 4&gt;() 1 #list comprehension to look at distribution for marriage vs default 3 [countplot_comparison(x ,10) for x in credit_df.columns if x.startswith(&#39;M&#39;)] -&gt; 4 plt.sj AttributeError: module &#39;matplotlib.pyplot&#39; has no attribute &#39;sj&#39; . Perhaps surprisingly it seems that people who are single are actually less likley to default on their payments. We know from the initial observations that the amount of single and married customers in this dataset is almost equal. Although the number of defaulters are equal for single and married people there is a higher portion of single people who pay on time next month i.e do not default on their payment. There is not enough information on people with &#39;other&#39; marital status.In fact it forms such small part of the overall dataset it is possible to drop it as its impact on the prediction models is likely to be insignificant. . def boxplot_comparison(feature1, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=target, y=feature1, data=credit_df, palette=&#39;pastel&#39;) plt.title(f&#39;{feature1} vs Default On Payment&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_comparison(&#39;LIMIT_BAL&#39;, 10) . The most obvious difference between the two is that customers who are likely to default have a significantly lower limit balance amount. These customers have already been partially identified as higher risk as they already have lower credit limits (in dollers). This column also includes family/supplementary credit suggesting that they have come from lower income backgrounds or thier limit has been based upon credit histories of thier family most probably parents. . Column: AGE . boxplot_comparison(&#39;AGE&#39;, 10) . There doesn&#39;t seem to be a huge differences in age between people who default and don&#39;t. The range of values is slightly larger for default which could mean the ages of people who don&#39;t pay on time is more spread out, with no set pattern. It is possible that people younger than 60, might be more likley to default as there is fewer outliers in the above 60 years old category compared to non-default which has many of over 60&#39;s (likely to be retired customers) who pay on time. However overall there doesn&#39;t seem to be much of link between them pureply by looking at the boxplot. . Column: PAY 1 . boxplot_comparison(&#39;PAY_1&#39;, 10) . This is where we really see the difference between defaulters and non-defaulters. The non-default column has a very small interquartile range , this is when the middle values are clustered more tightly as well as having close together min and max values. The IQR covers 75% of the data which suggests that the majority of people who don&#39;t default on thier payments tend to be late by only 2 months or 3 at the most barring outliers. . This plot suggests that people who have a higher number of months for thier repayment status column (PAY_1) on average are more likely to default on thier payments. The default values have a higher mediun of about 3, meaning that on average if someone is late on thier payments for 3 months or they will probably default on thier next months payment as well. Since the range of values is slightly larger, though there are few outliers, it does show that most defaulters tend to be late by 2-4 months. . Column: PAY_AMT1 . constant=1000 # from previous analysis we know we need to see this feature transformed so all the values are not affected by power law distribution pay_amt_log=credit_df[&#39;PAY_AMT1&#39;]+constant pay_amt_log=np.log(pay_amt_log) . boxplot_comparison(pay_amt_log, 10) . Thanks to transforming the data we can now clearly see that customers who default on thier payments, pay back noticablely less amounts in thier previous months statements than people who don&#39;t default. Although the median value is the same, the first quartile is the same as the minimum value and the third quartile is lower value for default. This suggests that customers are only able to pay back the minimum balance on their credit card debt every month. . Column: BILL_AMT1 . billamt_log=credit_df[&#39;BILL_AMT1&#39;]+constant billamt_log=np.log(billamt_log) boxplot_comparison(billamt_log, 10) . C: Users Robin anaconda3 lib site-packages pandas core arraylike.py:397: RuntimeWarning: divide by zero encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) C: Users Robin anaconda3 lib site-packages pandas core arraylike.py:397: RuntimeWarning: invalid value encountered in log result = getattr(ufunc, method)(*inputs, **kwargs) . There doesn&#39;t seem to be a big difference between the 2 for the bill amount column. This is unsurprising as we know from the correlation matrix that there is not much link between the credit bill amount to pay and the probability of the client paying. This does tell us that both sets of customers are spending roughly the same amount of money. However if we compare to the pay_amount column it suggests that one set of customers is spending more than they can afford to and is now struggling to pay back the banks for their expenditure. . . Summary of Findings . From the matrix we have found that PAY_1,2, and 3 followed by limit balance columns have the greatest correlation with the default column meaning they should be good indicators of weather someone defaults or not. Demographics wise it seems that single women with higher education are the least likely to default on payments. . People with lower limit balances based both on individuals and family credit as well as if they have higher score on the PAY columns are more likely to default. Similarly customers who have a lower previous payment amount but similar bill amounts to their counterparts are more likely to default as they are already struggling to pay back thier credit cards. . Reference . (Ref Brainstation - clustering-code-along-notebook) .",
            "url": "https://apayal.github.io/capstone_project/2022/08/07/EDA.html",
            "relUrl": "/2022/08/07/EDA.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Credit Card Payment Default",
            "content": "Notebook 1: Cleaning Data and Pre-Processing . Steps to load, clean, and analyze the data upon first inspection. . . Table of Contents . Introduction 1.1 Business Question 1.2 Data Collection . | Loading &amp; Checking 2.1 Data and Column Descriptions 2.2 Data Dictionary . | Investigate Columns 3.1.Correcting Column names 3.2.Clean unknown values . | Introduction . Business Question . Can we accurately predict the probability of a customer defaulting on their payments? . Context . Credit card are issued by banks or financial companies which allow clients to borrow money to pay for goods or services. Credit card companies maintain vast databases on cardholders and have the condition that cardholders pay back the company on time. Since the companies make the payment on the clients behalf it is in thier best interest that they get thier money back from the cardholders. . There are a number of ways the issuers makes thier profit but it mainly comes from annual fees to cardholders, transaction fees on purchases or transfers, processor fees but the largest profit portion comes from interest fees. Interest fees are charged when balance on the card is not paid on time. To read more on how these fees are calculated you can read more on the link. How do credit card companies make money . The bottom line is that the fees are calculated based on credit risk which is determined by credit history of individual. However with the case of this particular dataset the reality is quite different. . Data Background . In order to encourage the flow of capital, banks in Taiwan began extensively promoting the population to apply for credit cards. They lowered the requirements for approvals mainly targeting young people and students who lacked practical experience and financial knowledge, had no job or lower income. By February 2006 Taiwan had a credit card debt of $268 billion USD! This led to a range of social issues such as homelessness, violent debt collection especially towards lower income families and even suicide just because they were not able to pay off the debt. Until the government stepped in to sort the problems and implemented stricter rules that monitored who could be approved for credit cards as well as how much limit they can get. This is how we get our main problem statement and business motivation: . How can we accurately predict the probability of a customer defaulting especially when we have very limited to no credit history information? So, in this classification problem our stakeholders are credit card companies whose main motivation is not the accuracy of the prediction model but rather the recall rate. The recall measures the proportion of actual positives that were identified correctly. We are more interested in the number of false negatives being as low as possible and want the number of true positives to be high as possible. Basically, a customer who has been incorrectly predicted as likely to not default when in reality they are likely to default, poses the biggest risk to the bank. . Data collection . Normally credit card data is hard to find as it contains private personal details such as marital status, employment status etc. This particular dataset was donated to UCI Machine learning repository, however I initially found it on Kaggle.com. They are both the same but the original on UCI website has a fuller data dictionary of the two. . You can view the original data from here . Dataset Information . This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. . . Importing Libraries . First we must import libraries we need to clean and analyse the data. . import pandas as pd import numpy as np . . Loading &amp; Checking Data . Data and Column Description . Let&#39;s take a closer look at the data, before doing any analysis. . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data.csv&#39;) . print(f&#39;We have {credit_df.shape[0]} rows and {credit_df.shape[1]} columns in the dataset.&#39;) . We have 30000 rows and 25 columns in the dataset. . credit_df.head() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 11 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=0&#39;&gt;1&lt;/a&gt; #a snapshot of the first few rows -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=1&#39;&gt;2&lt;/a&gt; credit_df.head() NameError: name &#39;credit_df&#39; is not defined . credit_df.tail() # last 5 rows . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 29995 29996 | 220000 | 1 | 3 | 1 | 39 | 0 | 0 | 0 | 0 | ... | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | 0 | . 29996 29997 | 150000 | 1 | 3 | 2 | 43 | -1 | -1 | -1 | -1 | ... | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | 0 | . 29997 29998 | 30000 | 1 | 2 | 2 | 37 | 4 | 3 | 2 | -1 | ... | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | 1 | . 29998 29999 | 80000 | 1 | 3 | 1 | 41 | 1 | -1 | 0 | 0 | ... | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | 1 | . 29999 30000 | 50000 | 1 | 2 | 1 | 46 | 0 | 0 | 0 | 0 | ... | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | 1 | . 5 rows × 25 columns . . Data dictionary . Column Name Column Contents . ID&lt;/th&gt; ID of each client&lt;/th&gt; LIMIT_BAL&lt;/th&gt; Amount of given credit in NT dollars (includes individual and family/supplementary credit)&lt;/th&gt; &lt;/p&gt; SEX&lt;/th&gt; Gender (1=male, 2=female)&lt;/th&gt; &lt;/p&gt; EDUCATION&lt;/th&gt; Level of education(1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)&lt;/th&gt; &lt;/p&gt; MARRIAGE&lt;/th&gt; Marital status (1=married, 2=single, 3=others)&lt;/th&gt; &lt;/p&gt; AGE&lt;/th&gt; Age in years&lt;/th&gt; PAY_0&lt;/th&gt; Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)&lt;/th&gt; PAY_2&lt;/th&gt; Repayment status in August, 2005 (scale same as above)&lt;/th&gt; PAY_3&lt;/th&gt; Repayment status in July, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; PAY_4&lt;/th&gt; Repayment status in June, 2005 (scale same as above)&lt;/th&gt; PAY_5&lt;/th&gt; Repayment status in May, 2005 (scale same as above)&lt;/th&gt; PAY_6&lt;/th&gt; Repayment status in April, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; BILL_AMT1&lt;/th&gt; Amount of bill statement in September, 2005 (NT dollar)&lt;/th&gt; &lt;/p&gt; BILL_AMT2&lt;/th&gt; Amount of bill statement in August, 2005 (NT dollar)&lt;/th&gt; BILL_AMT3&lt;/th&gt; Amount of bill statement in July, 2005 (NT dollar)&lt;/th&gt; BILL_AMT4&lt;/th&gt; Amount of bill statement in June, 2005 (NT dollar)&lt;/th&gt; BILL_AMT5&lt;/th&gt; Amount of bill statement in May, 2005 (NT dollar)&lt;/th&gt; BILL_AMT6&lt;/th&gt; Amount of bill statement in April, 2005 (NT dollar)&lt;/th&gt; PAY_AMT1&lt;/th&gt; Amount of previous payment in September, 2005 (NT dollar)&lt;/th&gt; PAY_AMT2&lt;/th&gt; Amount of previous payment in August, 2005 (NT dollar)&lt;/th&gt; PAY_AMT3&lt;/th&gt; Amount of previous payment in July, 2005 (NT dollar)&lt;/th&gt; PAY_AMT4&lt;/th&gt; Amount of previous payment in June, 2005 (NT dollar)&lt;/th&gt; PAY_AMT5&lt;/th&gt; Amount of previous payment in May, 2005 (NT dollar)&lt;/th&gt; PAY_AMT6&lt;/th&gt; Amount of previous payment in April, 2005 (NT dollar)&lt;/th&gt; default.payment.next.month&lt;/th&gt; Default payment (1=yes, 0=no)&lt;/th&gt; &lt;/table&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Data cleaning and preprocessing . Dataset Summary Information . credit_df.isna().sum() . ID 0 LIMIT_BAL 0 SEX 0 EDUCATION 0 MARRIAGE 0 AGE 0 PAY_0 0 PAY_2 0 PAY_3 0 PAY_4 0 PAY_5 0 PAY_6 0 BILL_AMT1 0 BILL_AMT2 0 BILL_AMT3 0 BILL_AMT4 0 BILL_AMT5 0 BILL_AMT6 0 PAY_AMT1 0 PAY_AMT2 0 PAY_AMT3 0 PAY_AMT4 0 PAY_AMT5 0 PAY_AMT6 0 default payment next month 0 dtype: int64 . credit_df.duplicated().sum() . 0 . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to check all the column names are appropriate . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;PAY_0&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;default payment next month&#39;], dtype=&#39;object&#39;) . The columns PAY_0-PAY_6 are not very intuitive columns names. These columns represent the repayment history of the customer, so we can rename them to something like HIST_PAY1. The sixth column starts at PAY_0 but then skips to PAY_2. Looking at the other columns in a group such as BILL_AMT and PAY_AMT they all start at 1 and go up to 6. So we need to rename the column PAY_0 as PAY_1 so it has the same format as the other columns and avoid confusion. . Default payment next month is the target column i.e the one we want to predict. This column name should also be changed for easier access,reference and most importantly so it follows the same uniform naming convention as the other columns - 1 or 2 words in capitals that succinctly describes the data contained in the column. . new_column_names = {&#39;PAY_0&#39;: &#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;DEFAULT&#39; } . credit_df.rename(columns=new_column_names,inplace=True) . credit_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30000 entries, 0 to 29999 Data columns (total 25 columns): # Column Non-Null Count Dtype -- -- 0 ID 30000 non-null int64 1 LIMIT_BAL 30000 non-null int64 2 SEX 30000 non-null int64 3 EDUCATION 30000 non-null int64 4 MARRIAGE 30000 non-null int64 5 AGE 30000 non-null int64 6 PAY_1 30000 non-null int64 7 PAY_2 30000 non-null int64 8 PAY_3 30000 non-null int64 9 PAY_4 30000 non-null int64 10 PAY_5 30000 non-null int64 11 PAY_6 30000 non-null int64 12 BILL_AMT1 30000 non-null int64 13 BILL_AMT2 30000 non-null int64 14 BILL_AMT3 30000 non-null int64 15 BILL_AMT4 30000 non-null int64 16 BILL_AMT5 30000 non-null int64 17 BILL_AMT6 30000 non-null int64 18 PAY_AMT1 30000 non-null int64 19 PAY_AMT2 30000 non-null int64 20 PAY_AMT3 30000 non-null int64 21 PAY_AMT4 30000 non-null int64 22 PAY_AMT5 30000 non-null int64 23 PAY_AMT6 30000 non-null int64 24 DEFAULT 30000 non-null int64 dtypes: int64(25) memory usage: 5.7 MB . . Pre-Processing . Lets focus on cleaning and checking for inconsistencies in the demographic columns of the dataset first. . Column: SEX . From the data dictionary we already know that sex has the values 1 and 2 representing male and female. However in computing binary values are traditionally represented as 0 and 1, so we will change them. . credit_df[&#39;SEX&#39;].unique() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 25 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000025?line=0&#39;&gt;1&lt;/a&gt; credit_df[&#39;SEX&#39;].unique() NameError: name &#39;credit_df&#39; is not defined . credit_df[&#39;SEX&#39;]=credit_df[&#39;SEX&#39;].replace({1: 0, 2: 1}) . credit_df[&#39;SEX&#39;].value_counts() # check we only have 0 and 1 left . 1 18112 0 11888 Name: SEX, dtype: int64 . . Column: Education . Education also has some extra values. We already know there is 2 redundent values of 5 and 6. We have 4 to denote &#39;unknown&#39; or &#39;other&#39; education status. We can do a value counts to check if there are additional unknown values and if there are then we can also assign them to 4. . credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 . 2 46.766667 1 35.283333 3 16.390000 5 0.933333 4 0.410000 6 0.170000 0 0.046667 Name: EDUCATION, dtype: float64 . credit_df[&#39;EDUCATION&#39;]=credit_df[&#39;EDUCATION&#39;].replace([0,5,6], 4 ) credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 # now do get dummies so we only have 3 columns left . 2 46.766667 1 35.283333 3 16.390000 4 1.560000 Name: EDUCATION, dtype: float64 . There is actually 3 unknown values: 0,5,6. We can map these to the value 4 to refer to other either instituations or other unknown place of study. . However upon doing some external reseach, I found that Taiwan has a very high percentage of education completion in general. In fact around 94.7% of people have passed high school and gone onto some sort of higher education or senior vocational course . Based on this information, it is safe to assume that the unknown values and other category can be grouped together in the value 3, as it is highly likely that these people have passed high school at very the least. Additionally since adding the 3 unknown values to 4 gives us 468 values which accounts for less than 1.56% of the entire column it is insignificant in making predictions which means it can only contribute to making models more computationally expensive to compute. . education_df = pd.get_dummies(credit_df[&#39;EDUCATION&#39;],prefix=&#39;EDUCATION&#39;) . education_df.drop(columns=[&#39;EDUCATION_4&#39;], inplace=True) . education_df . Education_1 Education_2 Education_3 . 0 0 | 1 | 0 | . 1 0 | 1 | 0 | . 2 0 | 1 | 0 | . 3 0 | 1 | 0 | . 4 0 | 1 | 0 | . ... ... | ... | ... | . 29995 0 | 0 | 1 | . 29996 0 | 0 | 1 | . 29997 0 | 1 | 0 | . 29998 0 | 0 | 1 | . 29999 0 | 1 | 0 | . 30000 rows × 3 columns . education_df.value_counts() . Education_1 Education_2 Education_3 0 1 0 14030 1 0 0 10585 0 0 1 4917 0 468 dtype: int64 . . Column: Marriage . This also has 1 unknown value of 0, which we can change to 3 (others) . credit_df[&#39;MARRIAGE&#39;].value_counts() . 2 15964 1 13659 3 323 0 54 Name: MARRIAGE, dtype: int64 . credit_df[&#39;MARRIAGE&#39;]=credit_df[&#39;MARRIAGE&#39;].replace(0,3) . credit_df[&#39;MARRIAGE&#39;].value_counts(normalize=True)*100 . 2 53.213333 1 45.530000 3 1.256667 Name: MARRIAGE, dtype: float64 . Married and single can essentially be represented as 1 columns of married or not. Although value 3 only accounts for 1.25% of the column we can at the moment have a separate column for it but later when we do PCA or feature engineering I suspect it will be unimportant in predictions ans thus will be removed or not included in the modelling. . credit_df[&#39;MARRIAGE&#39;] . 0 1 1 2 2 2 3 1 4 1 .. 29995 1 29996 2 29997 2 29998 1 29999 1 Name: MARRIAGE, Length: 30000, dtype: int64 . marriage_df=pd.get_dummies(credit_df[&#39;MARRIAGE&#39;],prefix=&#39;Marital_status&#39;,drop_first=True) . marriage_df . Marital_status_2 Marital_status_3 . 0 0 | 0 | . 1 1 | 0 | . 2 1 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 29995 0 | 0 | . 29996 1 | 0 | . 29997 1 | 0 | . 29998 0 | 0 | . 29999 0 | 0 | . 30000 rows × 2 columns . We have removed the first column as the second column represents 0 for Married people and 1 for sinlge people. Maritial status_3 represents any another values. . . Putting back the cleaned data into 1 DataFrame . credit_df_clean = pd.concat([credit_df,education_df, marriage_df], axis=1) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_1 Education_2 Education_3 Marital_status_2 Marital_status_3 . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . rename_col = {&#39;Education_1&#39;: &#39;Education_higher&#39;, &#39;Education_2&#39;:&#39;Education_university&#39;, &#39;Education_3&#39;:&#39;Education_highschool&#39;, &#39;Marital_status_2&#39;:&#39;Marriage_Single&#39;, &#39;Marital_status_3&#39;:&#39;Marriage_Other&#39; } . credit_df_clean.rename(columns=rename_col,inplace=True) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . credit_df_clean=credit_df_clean.drop(columns=[&#39;EDUCATION&#39;,&#39;MARRIAGE&#39;]) . credit_df_clean . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5003 | 3047 | 5000 | 1000 | 0 | 0 | 0 | 1 | 0 | 0 | . 29996 29997 | 150000 | 0 | 43 | -1 | -1 | -1 | -1 | 0 | 0 | ... | 8998 | 129 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 29997 29998 | 30000 | 0 | 37 | 4 | 3 | 2 | -1 | 0 | 0 | ... | 22000 | 4200 | 2000 | 3100 | 1 | 0 | 1 | 0 | 1 | 0 | . 29998 29999 | 80000 | 0 | 41 | 1 | -1 | 0 | 0 | 0 | -1 | ... | 1178 | 1926 | 52964 | 1804 | 1 | 0 | 0 | 1 | 0 | 0 | . 29999 30000 | 50000 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1430 | 1000 | 1000 | 1000 | 1 | 0 | 1 | 0 | 0 | 0 | . 30000 rows × 28 columns . . Column: PAY columns values . The PAY_ columns range is actually -2 to 8 whereas in the dictionary it said -1 to 9. Let us make it 0-10 instead so all values are positive but the range and what it represents is unchanged. . credit_df_pay_col=credit_df_clean.columns[4:10] for month in credit_df_pay_col: print(sorted(credit_df_clean[month].unique())) . for month in credit_df_clean.columns[4:10]: credit_df_clean[month]=credit_df_clean[month]+2 # inplace=True) credit_df_clean[&#39;PAY_1&#39;].value_counts() . credit_df_clean.head() . . Saving the work . credit_df_clean.to_csv(&#39;credit_df_dataset_cleaned.csv&#39;, index=False) # the index column will not be saved . . Ending notes . In this notebook, I have mainly changed column names to more suitable names, reassign unknown values in column. Much of the data was originally categorical data that was transformed into ascending order values. However as this could be misinterpreted as values are ordinal. So re-encoding the categorical values into binary 0 and 1 was essential. . The next steps are to start doing some inital EDA on the dataset to see what initial insights we can gain. . . References . https://sevenpillarsinstitute.org/case-studies/taiwans-credit-card-crisis/ https://www.fool.com/the-ascent/credit-cards/how-do-credit-card-companies-make-money/ . &lt;/div&gt; . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | .",
            "url": "https://apayal.github.io/capstone_project/2022/08/07/Data_cleaning_pre-processing.html",
            "relUrl": "/2022/08/07/Data_cleaning_pre-processing.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://apayal.github.io/capstone_project/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://apayal.github.io/capstone_project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://apayal.github.io/capstone_project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}