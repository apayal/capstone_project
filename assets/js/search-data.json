{
  
    
        "post0": {
            "title": "Title",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data_categorical.csv&#39;) . credit_df . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | Female | University Graduate | Married | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | Female | University Graduate | Single | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | Female | University Graduate | Single | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | Female | University Graduate | Married | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | Male | University Graduate | Married | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | Male | High School | Married | 39 | 0 | 0 | 0 | 0 | ... | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | 0 | . 29996 29997 | 150000 | Male | High School | Single | 43 | -1 | -1 | -1 | -1 | ... | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | 0 | . 29997 29998 | 30000 | Male | University Graduate | Single | 37 | 4 | 3 | 2 | -1 | ... | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | 1 | . 29998 29999 | 80000 | Male | High School | Married | 41 | 1 | -1 | 0 | 0 | ... | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | 1 | . 29999 30000 | 50000 | Male | University Graduate | Married | 46 | 0 | 0 | 0 | 0 | ... | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | 1 | . 30000 rows × 25 columns . sns.histplot(credit_df[&#39;SEX&#39;]) . &lt;AxesSubplot:xlabel=&#39;SEX&#39;, ylabel=&#39;Count&#39;&gt; . sns.countplot(credit_df[&#39;SEX&#39;],palette=&quot;Set2&quot;) . C: Users Robin anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;SEX&#39;, ylabel=&#39;count&#39;&gt; . sns.countplot(credit_df[&#39;MARRIAGE&#39;],palette=&quot;Set2&quot;) . C: Users Robin anaconda3 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( . &lt;AxesSubplot:xlabel=&#39;MARRIAGE&#39;, ylabel=&#39;count&#39;&gt; . education_percentage=credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 . education_percentage.index . Index([&#39;University Graduate&#39;, &#39;Graduate school&#39;, &#39;High school&#39;, &#39;Unknown&#39;, &#39;other&#39;, &#39;not known&#39;, &#39;not known 1&#39;], dtype=&#39;object&#39;) . plt.figure(figsize=(12,7)) sns.barplot(x=education_percentage.index,y=education_percentage.values,palette=&#39;muted&#39;) plt.ylabel(&#39;Percentage&#39;) plt.title(&#39;Percentage of Education&#39;) plt.xlabel(&#39;Education&#39;) plt.show() . education_percentage.plot(kind=&#39;bar&#39;) . &lt;AxesSubplot:&gt; . credit_df[&#39;EDUCATION&#39;].value_counts() . University Graduate 14030 Graduate school 10585 High school 4917 Unknown 280 other institutions 123 Not recognised 51 Not recognised 1 14 Name: EDUCATION, dtype: int64 . # Marriage, Age, and Sex def boxplot_variation(feature1, feature2, feature3, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=feature1, y=feature2, hue=feature3, data=credit_df, palette=&#39;pastel&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_variation(&#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;SEX&#39;,10) . credit_df[&#39;LIMIT_BAL&#39;].value_counts() . 50000 3365 20000 1976 30000 1610 80000 1567 200000 1528 ... 730000 2 1000000 1 327680 1 760000 1 690000 1 Name: LIMIT_BAL, Length: 81, dtype: int64 . plt.figure(figsize = (14,6)) plt.title(&#39;Amount of credit limit &#39;) sns.histplot(credit_df[&#39;LIMIT_BAL&#39;],bins=40) plt.show() . plt.figure(figsize = (14,6)) plt.title(&#39;Amount of credit limit - Density Plot&#39;) sns.set_color_codes(&quot;pastel&quot;) sns.distplot(credit_df[&#39;LIMIT_BAL&#39;],kde=True,bins=200) plt.show() . C: Users Robin anaconda3 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) .",
            "url": "https://apayal.github.io/capstone_project/2022/08/02/same.html",
            "relUrl": "/2022/08/02/same.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Modelling . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns #modelling from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data credit-df-dataset-cleaned.csv&#39;) . We will be using log transformed data as . credit_df_log=credit_df.iloc[:,4:22] credit_df_log . PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 . 0 3 | 3 | 0 | 0 | -1 | -1 | 3913 | 3102 | 689 | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | . 1 0 | 3 | 1 | 1 | 1 | 3 | 2682 | 1725 | 2682 | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | . 2 1 | 1 | 1 | 1 | 1 | 1 | 29239 | 14027 | 13559 | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | . 3 1 | 1 | 1 | 1 | 1 | 1 | 46990 | 48233 | 49291 | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | . 4 0 | 1 | 0 | 1 | 1 | 1 | 8617 | 5670 | 35835 | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 1 | 1 | 1 | 1 | 1 | 1 | 188948 | 192815 | 208365 | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | . 29996 0 | 0 | 0 | 0 | 1 | 1 | 1683 | 1828 | 3502 | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | . 29997 5 | 4 | 3 | 0 | 1 | 1 | 3565 | 3356 | 2758 | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | . 29998 2 | 0 | 1 | 1 | 1 | 0 | -1645 | 78379 | 76304 | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | . 29999 1 | 1 | 1 | 1 | 1 | 1 | 47929 | 48905 | 49764 | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | . 30000 rows × 18 columns . c=np.log(credit_df_log) . c: Users Robin anaconda3 lib site-packages pandas core internals blocks.py:402: RuntimeWarning: divide by zero encountered in log result = func(self.values, **kwargs) c: Users Robin anaconda3 lib site-packages pandas core internals blocks.py:402: RuntimeWarning: invalid value encountered in log result = func(self.values, **kwargs) . # plt.figure() # plt.title(f&#39;Feature: {col}&#39;) # sns.histplot(c[col],bins=30) # plt.show() sns.histplot(c[&#39;BILL_AMT1&#39;]) # seems to be more noramlly distributed now . &lt;AxesSubplot:xlabel=&#39;BILL_AMT1&#39;, ylabel=&#39;Count&#39;&gt; . demo_data=credit_df.iloc[:,0:4] log_df=pd.concat([demo_data,c],axis=1) . log_df . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 . 0 1 | 20000 | 1 | 24 | 1.098612 | 1.098612 | -inf | -inf | NaN | NaN | ... | 6.535241 | -inf | -inf | -inf | -inf | 6.535241 | -inf | -inf | -inf | -inf | . 1 2 | 120000 | 1 | 26 | -inf | 1.098612 | 0.000000 | 0.0 | 0.0 | 1.098612 | ... | 7.894318 | 8.093157 | 8.147578 | 8.089789 | -inf | 6.907755 | 6.907755 | 6.907755 | -inf | 7.600902 | . 2 3 | 90000 | 1 | 34 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | ... | 9.514806 | 9.570180 | 9.612333 | 9.651752 | 7.325149 | 7.313220 | 6.907755 | 6.907755 | 6.907755 | 8.517193 | . 3 4 | 50000 | 1 | 37 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | ... | 10.805497 | 10.251112 | 10.273636 | 10.293737 | 7.600902 | 7.610358 | 7.090077 | 7.003065 | 6.974479 | 6.907755 | . 4 5 | 50000 | 0 | 57 | -inf | 0.000000 | -inf | 0.0 | 0.0 | 0.000000 | ... | 10.486680 | 9.949416 | 9.859849 | 9.859065 | 7.600902 | 10.510014 | 9.210340 | 9.104980 | 6.535241 | 6.520621 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | ... | 12.247047 | 11.385138 | 10.349359 | 9.679093 | 9.047821 | 9.903488 | 8.517793 | 8.021913 | 8.517193 | 6.907755 | . 29996 29997 | 150000 | 0 | 43 | -inf | -inf | -inf | -inf | 0.0 | 0.000000 | ... | 8.161090 | 9.102644 | 8.554489 | -inf | 7.515889 | 8.167919 | 9.104758 | 4.859812 | -inf | -inf | . 29997 29998 | 30000 | 0 | 37 | 1.609438 | 1.386294 | 1.098612 | -inf | 0.0 | 0.000000 | ... | 7.922261 | 9.946451 | 9.932172 | 9.870809 | -inf | -inf | 9.998798 | 8.342840 | 7.600902 | 8.039157 | . 29998 29999 | 80000 | 0 | 41 | 0.693147 | -inf | 0.000000 | 0.0 | 0.0 | -inf | ... | 11.242481 | 10.873774 | 9.380505 | 10.798432 | 11.360939 | 8.134174 | 7.071573 | 7.563201 | 10.877368 | 7.497762 | . 29999 30000 | 50000 | 0 | 46 | 0.000000 | 0.000000 | 0.000000 | 0.0 | 0.0 | 0.000000 | ... | 10.815047 | 10.506026 | 10.386778 | 9.636457 | 7.639161 | 7.495542 | 7.265430 | 6.907755 | 6.907755 | 6.907755 | . 30000 rows × 22 columns . y=credit_df[&#39;DEFAULT&#39;] X=credit_df.loc[:, credit_df.columns != &#39;DEFAULT&#39;] . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score,plot_confusion_matrix,classification_report model1 = LogisticRegression() model1.fit(X_train_scaled, y_train) y_pred = model1.predict(X_test) print(classification_report(y_pred, y_test)) print(confusion_matrix(y_pred, y_test)) print(&#39; nAccuracy Score for model1 test: &#39;, accuracy_score(y_pred,y_test)) . precision recall f1-score support 0 1.00 0.78 0.88 5995 1 0.00 0.20 0.00 5 accuracy 0.78 6000 macro avg 0.50 0.49 0.44 6000 weighted avg 1.00 0.78 0.88 6000 [[4679 1316] [ 4 1]] Accuracy Score for model1 test: 0.78 . c: Users Robin anaconda3 lib site-packages sklearn base.py:443: UserWarning: X has feature names, but LogisticRegression was fitted without feature names warnings.warn( . . 0.0007593014426727411 . model1.score(X_train_scaled,y_train) . 0.8099166666666666 . lr=model1.score(X_train_scaled,y_train) . It&#39;s clear that there is a bias towards the not default class. We will need to balance out the class by upsampling to get better results. . import imblearn print(imblearn.__version__) # check it was installed properly . 0.9.1 . from imblearn.over_sampling import SMOTE oversample=SMOTE() X,y = oversample.fit_resample(X,y) . predictors = credit_df.loc[:, credit_df.columns != &#39;DEFAULT&#39;] oversample=SMOTE() predictors,y = oversample.fit_resample(predictors,y) X_train, X_test, y_train, y_test = train_test_split(predictors, y, test_size=0.2, random_state=10) model2=LogisticRegression() model2.fit(X_train_scaled, y_train) y_pred2 = model2.predict(X_test) print(classification_report(y_pred2, y_test)) print(confusion_matrix(y_pred, y_test)) . ValueError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks modelling.ipynb Cell 19 in &lt;cell line: 3&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000028?line=0&#39;&gt;1&lt;/a&gt; predictors = credit_df.loc[:, credit_df.columns != &#39;DEFAULT&#39;] &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000028?line=1&#39;&gt;2&lt;/a&gt; oversample=SMOTE() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000028?line=2&#39;&gt;3&lt;/a&gt; predictors,y = oversample.fit_resample(predictors,y) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000028?line=4&#39;&gt;5&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(predictors, y, test_size=0.2, random_state=10) &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000028?line=5&#39;&gt;6&lt;/a&gt; model2=LogisticRegression() File c: Users Robin anaconda3 lib site-packages imblearn base.py:77, in SamplerMixin.fit_resample(self, X, y) 75 check_classification_targets(y) 76 arrays_transformer = ArraysTransformer(X, y) &gt; 77 X, y, binarize_y = self._check_X_y(X, y) 79 self.sampling_strategy_ = check_sampling_strategy( 80 self.sampling_strategy, y, self._sampling_type 81 ) 83 output = self._fit_resample(X, y) File c: Users Robin anaconda3 lib site-packages imblearn base.py:132, in BaseSampler._check_X_y(self, X, y, accept_sparse) 130 accept_sparse = [&#34;csr&#34;, &#34;csc&#34;] 131 y, binarize_y = check_target_type(y, indicate_one_vs_all=True) --&gt; 132 X, y = self._validate_data(X, y, reset=True, accept_sparse=accept_sparse) 133 return X, y, binarize_y File c: Users Robin anaconda3 lib site-packages sklearn base.py:596, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, **check_params) 594 y = check_array(y, input_name=&#34;y&#34;, **check_y_params) 595 else: --&gt; 596 X, y = check_X_y(X, y, **check_params) 597 out = X, y 599 if not no_val_X and check_params.get(&#34;ensure_2d&#34;, True): File c: Users Robin anaconda3 lib site-packages sklearn utils validation.py:1092, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator) 1074 X = check_array( 1075 X, 1076 accept_sparse=accept_sparse, (...) 1087 input_name=&#34;X&#34;, 1088 ) 1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator) -&gt; 1092 check_consistent_length(X, y) 1094 return X, y File c: Users Robin anaconda3 lib site-packages sklearn utils validation.py:387, in check_consistent_length(*arrays) 385 uniques = np.unique(lengths) 386 if len(uniques) &gt; 1: --&gt; 387 raise ValueError( 388 &#34;Found input variables with inconsistent numbers of samples: %r&#34; 389 % [int(l) for l in lengths] 390 ) ValueError: Found input variables with inconsistent numbers of samples: [30000, 46728] . from sklearn.neighbors import KNeighborsClassifier knc = KNeighborsClassifier(n_neighbors=4) knc.fit(X_train_scaled,y_train) Y_pred_knc = knc.predict(X_test_scaled) accuracy_knc = accuracy_score(y_test,Y_pred_knc) accuracy_knc . 0.7953333333333333 . recall_score(X_test_scaled, y_test) . ValueError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks modelling.ipynb Cell 13 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000009?line=0&#39;&gt;1&lt;/a&gt; # recall_score(true labels, predicted labels) -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000009?line=1&#39;&gt;2&lt;/a&gt; recall_score(X_test_scaled, y_test) File c: Users Robin anaconda3 lib site-packages sklearn metrics _classification.py:1901, in recall_score(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division) 1770 def recall_score( 1771 y_true, 1772 y_pred, (...) 1778 zero_division=&#34;warn&#34;, 1779 ): 1780 &#34;&#34;&#34;Compute the recall. 1781 1782 The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of (...) 1899 array([1. , 1. , 0.5]) 1900 &#34;&#34;&#34; -&gt; 1901 _, r, _, _ = precision_recall_fscore_support( 1902 y_true, 1903 y_pred, 1904 labels=labels, 1905 pos_label=pos_label, 1906 average=average, 1907 warn_for=(&#34;recall&#34;,), 1908 sample_weight=sample_weight, 1909 zero_division=zero_division, 1910 ) 1911 return r File c: Users Robin anaconda3 lib site-packages sklearn metrics _classification.py:1544, in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division) 1542 if beta &lt; 0: 1543 raise ValueError(&#34;beta should be &gt;=0 in the F-beta score&#34;) -&gt; 1544 labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) 1546 # Calculate tp_sum, pred_sum, true_sum ### 1547 samplewise = average == &#34;samples&#34; File c: Users Robin anaconda3 lib site-packages sklearn metrics _classification.py:1348, in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label) 1345 if average not in average_options and average != &#34;binary&#34;: 1346 raise ValueError(&#34;average has to be one of &#34; + str(average_options)) -&gt; 1348 y_type, y_true, y_pred = _check_targets(y_true, y_pred) 1349 # Convert to Python primitive type to avoid NumPy type / Python str 1350 # comparison. See https://github.com/numpy/numpy/issues/6784 1351 present_labels = unique_labels(y_true, y_pred).tolist() File c: Users Robin anaconda3 lib site-packages sklearn metrics _classification.py:93, in _check_targets(y_true, y_pred) 90 y_type = {&#34;multiclass&#34;} 92 if len(y_type) &gt; 1: &gt; 93 raise ValueError( 94 &#34;Classification metrics can&#39;t handle a mix of {0} and {1} targets&#34;.format( 95 type_true, type_pred 96 ) 97 ) 99 # We can&#39;t have more than one value on y_type =&gt; The set is no more needed 100 y_type = y_type.pop() ValueError: Classification metrics can&#39;t handle a mix of continuous-multioutput and binary targets . knc.score(X_train_scaled,y_train) . 0.83975 . from sklearn.metrics import plot_confusion_matrix plot_confusion_matrix(Y_pred_knc, y_test) . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks modelling.ipynb Cell 21 in &lt;cell line: 4&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000015?line=0&#39;&gt;1&lt;/a&gt; from sklearn.metrics import plot_confusion_matrix -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000015?line=3&#39;&gt;4&lt;/a&gt; plot_confusion_matrix(Y_pred_knc, y_test) NameError: name &#39;Y_pred_knc&#39; is not defined . from sklearn.svm import LinearSVC SVM_model = LinearSVC() SVM_model.fit(X_train_scaled, y_train) Y_pred_svm = SVM_model.predict(X_test_scaled) print(f&quot;The TRAIN classification accuracy is: {SVM_model.score(X_train, y_train)}&quot;) recall_score(y_test, Y_pred_svm) accuracy_svm = accuracy_score(y_test,Y_pred_svm) accuracy_svm . counter=Counter(y) . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks modelling.ipynb Cell 19 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/modelling.ipynb#ch0000018?line=0&#39;&gt;1&lt;/a&gt; counter=Counter(y) NameError: name &#39;Counter&#39; is not defined .",
            "url": "https://apayal.github.io/capstone_project/2022/08/02/modelling.html",
            "relUrl": "/2022/08/02/modelling.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Predicting Credit Card Payment Default",
            "content": "Notebook 1: Loading and Cleaning Data . Steps to load, clean, and analyze the data upon first inspection. . . Table of Contents . Introduction 1.1 Problem Statement 1.2 Data Collection . | Loading &amp; Checking 2.1 Data and Column Descriptions 2.2 Data Dictionary . | Investigate Columns 3.1.Correcting Column names 3.2.Clean unknown values . | Introduction . Business Question . Can we accurately predict the probability of a customer defaulting on their payments? . Data collection . Normally credit risk data is hard to find as it contains private personal details such as marital status, employment status etc. This particular dataset was donated to UCI Machine learning repository, however I initially found it on Kaggle.com. They are both the same but the original on UCI website has a fuller data dictionary of the two. . You can view the original data from here . Dataset Information . This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. . . Importing Libraries . First we must import libraries we need to clean and analyse the data. . import pandas as pd import numpy as np . . Loading &amp; Checking Data . Data and Column Description . Let&#39;s take a closer look at the data, before doing any analysis. . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data.csv&#39;) . print(f&#39;We have {credit_df.shape[0]} rows and {credit_df.shape[1]} columns in the dataset.&#39;) . We have 30000 rows and 25 columns in the dataset. . credit_df.head() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 11 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=0&#39;&gt;1&lt;/a&gt; #a snapshot of the first few rows -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000012?line=1&#39;&gt;2&lt;/a&gt; credit_df.head() NameError: name &#39;credit_df&#39; is not defined . credit_df.tail() # last 5 rows . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 29995 29996 | 220000 | 1 | 3 | 1 | 39 | 0 | 0 | 0 | 0 | ... | 88004 | 31237 | 15980 | 8500 | 20000 | 5003 | 3047 | 5000 | 1000 | 0 | . 29996 29997 | 150000 | 1 | 3 | 2 | 43 | -1 | -1 | -1 | -1 | ... | 8979 | 5190 | 0 | 1837 | 3526 | 8998 | 129 | 0 | 0 | 0 | . 29997 29998 | 30000 | 1 | 2 | 2 | 37 | 4 | 3 | 2 | -1 | ... | 20878 | 20582 | 19357 | 0 | 0 | 22000 | 4200 | 2000 | 3100 | 1 | . 29998 29999 | 80000 | 1 | 3 | 1 | 41 | 1 | -1 | 0 | 0 | ... | 52774 | 11855 | 48944 | 85900 | 3409 | 1178 | 1926 | 52964 | 1804 | 1 | . 29999 30000 | 50000 | 1 | 2 | 1 | 46 | 0 | 0 | 0 | 0 | ... | 36535 | 32428 | 15313 | 2078 | 1800 | 1430 | 1000 | 1000 | 1000 | 1 | . 5 rows × 25 columns . . Data dictionary . Column Name Column Contents . ID&lt;/th&gt; ID of each client&lt;/th&gt; LIMIT_BAL&lt;/th&gt; Amount of given credit in NT dollars (includes individual and family/supplementary credit)&lt;/th&gt; &lt;/p&gt; SEX&lt;/th&gt; Gender (1=male, 2=female)&lt;/th&gt; &lt;/p&gt; EDUCATION&lt;/th&gt; Level of education(1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)&lt;/th&gt; &lt;/p&gt; MARRIAGE&lt;/th&gt; Marital status (1=married, 2=single, 3=others)&lt;/th&gt; &lt;/p&gt; AGE&lt;/th&gt; Age in years&lt;/th&gt; PAY_0&lt;/th&gt; Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)&lt;/th&gt; PAY_2&lt;/th&gt; Repayment status in August, 2005 (scale same as above)&lt;/th&gt; PAY_3&lt;/th&gt; Repayment status in July, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; PAY_4&lt;/th&gt; Repayment status in June, 2005 (scale same as above)&lt;/th&gt; PAY_5&lt;/th&gt; Repayment status in May, 2005 (scale same as above)&lt;/th&gt; PAY_6&lt;/th&gt; Repayment status in April, 2005 (scale same as above)&lt;/th&gt; &lt;/p&gt; BILL_AMT1&lt;/th&gt; Amount of bill statement in September, 2005 (NT dollar)&lt;/th&gt; &lt;/p&gt; BILL_AMT2&lt;/th&gt; Amount of bill statement in August, 2005 (NT dollar)&lt;/th&gt; BILL_AMT3&lt;/th&gt; Amount of bill statement in July, 2005 (NT dollar)&lt;/th&gt; BILL_AMT4&lt;/th&gt; Amount of bill statement in June, 2005 (NT dollar)&lt;/th&gt; BILL_AMT5&lt;/th&gt; Amount of bill statement in May, 2005 (NT dollar)&lt;/th&gt; BILL_AMT6&lt;/th&gt; Amount of bill statement in April, 2005 (NT dollar)&lt;/th&gt; PAY_AMT1&lt;/th&gt; Amount of previous payment in September, 2005 (NT dollar)&lt;/th&gt; PAY_AMT2&lt;/th&gt; Amount of previous payment in August, 2005 (NT dollar)&lt;/th&gt; PAY_AMT3&lt;/th&gt; Amount of previous payment in July, 2005 (NT dollar)&lt;/th&gt; PAY_AMT4&lt;/th&gt; Amount of previous payment in June, 2005 (NT dollar)&lt;/th&gt; PAY_AMT5&lt;/th&gt; Amount of previous payment in May, 2005 (NT dollar)&lt;/th&gt; PAY_AMT6&lt;/th&gt; Amount of previous payment in April, 2005 (NT dollar)&lt;/th&gt; default.payment.next.month&lt;/th&gt; Default payment (1=yes, 0=no)&lt;/th&gt; &lt;/table&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Data cleaning and preprocessing . Dataset Summary Information . credit_df.isna().sum() . ID 0 LIMIT_BAL 0 SEX 0 EDUCATION 0 MARRIAGE 0 AGE 0 PAY_0 0 PAY_2 0 PAY_3 0 PAY_4 0 PAY_5 0 PAY_6 0 BILL_AMT1 0 BILL_AMT2 0 BILL_AMT3 0 BILL_AMT4 0 BILL_AMT5 0 BILL_AMT6 0 PAY_AMT1 0 PAY_AMT2 0 PAY_AMT3 0 PAY_AMT4 0 PAY_AMT5 0 PAY_AMT6 0 default payment next month 0 dtype: int64 . credit_df.duplicated().sum() . 0 . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to check all the column names are appropriate . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;PAY_0&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;default payment next month&#39;], dtype=&#39;object&#39;) . The columns PAY_0-PAY_6 are not very intuitive columns names. These columns represent the repayment history of the customer, so we can rename them to something like HIST_PAY1. The sixth column starts at PAY_0 but then skips to PAY_2. Looking at the other columns in a group such as BILL_AMT and PAY_AMT they all start at 1 and go up to 6. So we need to rename the column PAY_0 as PAY_1 so it has the same format as the other columns and avoid confusion. . Default payment next month is the target column i.e the one we want to predict. This column name should also be changed for easier access,reference and most importantly so it follows the same uniform naming convention as the other columns - 1 or 2 words in capitals that succinctly describes the data contained in the column. . new_column_names = {&#39;PAY_0&#39;: &#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;DEFAULT&#39; } . credit_df.rename(columns=new_column_names,inplace=True) . credit_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30000 entries, 0 to 29999 Data columns (total 25 columns): # Column Non-Null Count Dtype -- -- 0 ID 30000 non-null int64 1 LIMIT_BAL 30000 non-null int64 2 SEX 30000 non-null int64 3 EDUCATION 30000 non-null int64 4 MARRIAGE 30000 non-null int64 5 AGE 30000 non-null int64 6 PAY_1 30000 non-null int64 7 PAY_2 30000 non-null int64 8 PAY_3 30000 non-null int64 9 PAY_4 30000 non-null int64 10 PAY_5 30000 non-null int64 11 PAY_6 30000 non-null int64 12 BILL_AMT1 30000 non-null int64 13 BILL_AMT2 30000 non-null int64 14 BILL_AMT3 30000 non-null int64 15 BILL_AMT4 30000 non-null int64 16 BILL_AMT5 30000 non-null int64 17 BILL_AMT6 30000 non-null int64 18 PAY_AMT1 30000 non-null int64 19 PAY_AMT2 30000 non-null int64 20 PAY_AMT3 30000 non-null int64 21 PAY_AMT4 30000 non-null int64 22 PAY_AMT5 30000 non-null int64 23 PAY_AMT6 30000 non-null int64 24 DEFAULT 30000 non-null int64 dtypes: int64(25) memory usage: 5.7 MB . . Clean unknown values . Lets focus on cleaning and checking for inconsistencies in the demographic columns of the dataset first. . Column: SEX . From the data dictionary we already know that sex has the values 1 and 2 representing male and female. However in computing binary values are traditionally represented as 0 and 1, so we will change them. . credit_df[&#39;SEX&#39;].unique() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks Data_cleaning.ipynb Cell 25 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/Data_cleaning.ipynb#ch0000025?line=0&#39;&gt;1&lt;/a&gt; credit_df[&#39;SEX&#39;].unique() NameError: name &#39;credit_df&#39; is not defined . credit_df[&#39;SEX&#39;]=credit_df[&#39;SEX&#39;].replace({1: 0, 2: 1}) . credit_df[&#39;SEX&#39;].value_counts() # check we only have 0 and 1 left . 1 18112 0 11888 Name: SEX, dtype: int64 . . Column: Education . Education also has some extra values. We already know there is 2 redundent values of 5 and 6. We have 4 to denote &#39;unknown&#39; or &#39;other&#39; education status. We can do a value counts to check if there are additional unknown values and if there are then we can also assign them to 4. . credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 . 2 46.766667 1 35.283333 3 16.390000 5 0.933333 4 0.410000 6 0.170000 0 0.046667 Name: EDUCATION, dtype: float64 . credit_df[&#39;EDUCATION&#39;]=credit_df[&#39;EDUCATION&#39;].replace([0,5,6], 4 ) credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 # now do get dummies so we only have 3 columns left . 2 46.766667 1 35.283333 3 16.390000 4 1.560000 Name: EDUCATION, dtype: float64 . There is actually 3 unknown values: 0,5,6. We can map these to the value 4 to refer to other either instituations or other unknown place of study. . However upon doing some external reseach, I found that Taiwan has a very high percentage of education completion in general. In fact around 94.7% of people have passed high school and gone onto some sort of higher education or senior vocational course . Based on this information, it is safe to assume that the unknown values and other category can be grouped together in the value 3, as it is highly likely that these people have passed high school at very the least. Additionally since adding the 3 unknown values to 4 gives us 468 values which accounts for less than 1.56% of the entire column it is insignificant in making predictions which means it can only contribute to making models more computationally expensive to compute. . education_df = pd.get_dummies(credit_df[&#39;EDUCATION&#39;],prefix=&#39;EDUCATION&#39;) . education_df.drop(columns=[&#39;EDUCATION_4&#39;], inplace=True) . education_df . Education_1 Education_2 Education_3 . 0 0 | 1 | 0 | . 1 0 | 1 | 0 | . 2 0 | 1 | 0 | . 3 0 | 1 | 0 | . 4 0 | 1 | 0 | . ... ... | ... | ... | . 29995 0 | 0 | 1 | . 29996 0 | 0 | 1 | . 29997 0 | 1 | 0 | . 29998 0 | 0 | 1 | . 29999 0 | 1 | 0 | . 30000 rows × 3 columns . education_df.value_counts() . Education_1 Education_2 Education_3 0 1 0 14030 1 0 0 10585 0 0 1 4917 0 468 dtype: int64 . . Column: Marriage . This also has 1 unknown value of 0, which we can change to 3 (others) . credit_df[&#39;MARRIAGE&#39;].value_counts() . 2 15964 1 13659 3 323 0 54 Name: MARRIAGE, dtype: int64 . credit_df[&#39;MARRIAGE&#39;]=credit_df[&#39;MARRIAGE&#39;].replace(0,3) . credit_df[&#39;MARRIAGE&#39;].value_counts(normalize=True)*100 . 2 53.213333 1 45.530000 3 1.256667 Name: MARRIAGE, dtype: float64 . Married and single can essentially be represented as 1 columns of married or not. Although value 3 only accounts for 1.25% of the column we can at the moment have a separate column for it but later when we do PCA or feature engineering I suspect it will be unimportant in predictions ans thus will be removed or not included in the modelling. . credit_df[&#39;MARRIAGE&#39;] . 0 1 1 2 2 2 3 1 4 1 .. 29995 1 29996 2 29997 2 29998 1 29999 1 Name: MARRIAGE, Length: 30000, dtype: int64 . marriage_df=pd.get_dummies(credit_df[&#39;MARRIAGE&#39;],prefix=&#39;Marital_status&#39;,drop_first=True) . marriage_df . Marital_status_2 Marital_status_3 . 0 0 | 0 | . 1 1 | 0 | . 2 1 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 29995 0 | 0 | . 29996 1 | 0 | . 29997 1 | 0 | . 29998 0 | 0 | . 29999 0 | 0 | . 30000 rows × 2 columns . We have removed the first column as the second column represents 0 for Married people and 1 for sinlge people. Maritial status_3 represents any another values. . . Putting back the cleaned data into 1 DataFrame . credit_df_clean = pd.concat([credit_df,education_df, marriage_df], axis=1) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_1 Education_2 Education_3 Marital_status_2 Marital_status_3 . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . rename_col = {&#39;Education_1&#39;: &#39;Education_higher&#39;, &#39;Education_2&#39;:&#39;Education_university&#39;, &#39;Education_3&#39;:&#39;Education_highschool&#39;, &#39;Marital_status_2&#39;:&#39;Marriage_Single&#39;, &#39;Marital_status_3&#39;:&#39;Marriage_Other&#39; } . credit_df_clean.rename(columns=rename_col,inplace=True) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . credit_df_clean=credit_df_clean.drop(columns=[&#39;EDUCATION&#39;,&#39;MARRIAGE&#39;]) . credit_df_clean . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5003 | 3047 | 5000 | 1000 | 0 | 0 | 0 | 1 | 0 | 0 | . 29996 29997 | 150000 | 0 | 43 | -1 | -1 | -1 | -1 | 0 | 0 | ... | 8998 | 129 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 29997 29998 | 30000 | 0 | 37 | 4 | 3 | 2 | -1 | 0 | 0 | ... | 22000 | 4200 | 2000 | 3100 | 1 | 0 | 1 | 0 | 1 | 0 | . 29998 29999 | 80000 | 0 | 41 | 1 | -1 | 0 | 0 | 0 | -1 | ... | 1178 | 1926 | 52964 | 1804 | 1 | 0 | 0 | 1 | 0 | 0 | . 29999 30000 | 50000 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1430 | 1000 | 1000 | 1000 | 1 | 0 | 1 | 0 | 0 | 0 | . 30000 rows × 28 columns . . Column: PAY columns values . The PAY_ columns range is actually -2 to 8 whereas in the dictionary it said -1 to 9. Let us make it 0-10 instead so all values are positive but the range and what it represents is unchanged. . credit_df_pay_col=credit_df_clean.columns[4:10] for month in credit_df_pay_col: print(sorted(credit_df_clean[month].unique())) . for month in credit_df_clean.columns[4:10]: credit_df_clean[month]=credit_df_clean[month]+2 # inplace=True) credit_df_clean[&#39;PAY_1&#39;].value_counts() . credit_df_clean.head() . . Saving the work . #credit_df_clean.to_csv(&#39;credit_df_dataset_cleaned.csv&#39;, index=False) # the index column will not be saved . . Ending notes . In this notebook, I have mainly changed column names to more suitable names, reassign unknown values in column. Much of the data was originally categorical data that was transformed into ascending order values. However as this could be misinterpreted as values are ordinal. So re-encoding the categorical values into binary 0 and 1 was essential. . The next steps are to start doing some inital EDA on the dataset to see what initial insights we can gain. . . &lt;/div&gt; . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | . | | .",
            "url": "https://apayal.github.io/capstone_project/2022/08/02/Data_cleaning.html",
            "relUrl": "/2022/08/02/Data_cleaning.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Predicting credit card defaults",
            "content": "Notebook 2 - EDA . Exploring the data . Table of Contents . Exploratory Data Analysis 1.1 Overview of Data 1.2 Data and Column Descriptions 1.3 Data Dictionary | import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data credit_df_dataset_cleaned.csv&#39;) . Overview of the data . credit_df.head() . NameError Traceback (most recent call last) c: Users Robin Downloads capstone_project _notebooks 2022-07-01-eda.ipynb Cell 6 in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/2022-07-01-eda.ipynb#ch0000005?line=0&#39;&gt;1&lt;/a&gt; #look at the columns and rows again to remind what were working with -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/Robin/Downloads/capstone_project/_notebooks/2022-07-01-eda.ipynb#ch0000005?line=1&#39;&gt;2&lt;/a&gt; credit_df.head() NameError: name &#39;credit_df&#39; is not defined . credit_df.describe() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 3.000000e+04 | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 15000.500000 | 167484.322667 | 1.603733 | 1.853133 | 1.551867 | 35.485500 | -0.016700 | -0.133767 | -0.166200 | -0.220667 | ... | 43262.948967 | 40311.400967 | 38871.760400 | 5663.580500 | 5.921163e+03 | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | . std 8660.398374 | 129747.661567 | 0.489129 | 0.790349 | 0.521970 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | ... | 64332.856134 | 60797.155770 | 59554.107537 | 16563.280354 | 2.304087e+04 | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | . min 1.000000 | 10000.000000 | 1.000000 | 0.000000 | 0.000000 | 21.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | ... | -170000.000000 | -81334.000000 | -339603.000000 | 0.000000 | 0.000000e+00 | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 7500.750000 | 50000.000000 | 1.000000 | 1.000000 | 1.000000 | 28.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 2326.750000 | 1763.000000 | 1256.000000 | 1000.000000 | 8.330000e+02 | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | . 50% 15000.500000 | 140000.000000 | 2.000000 | 2.000000 | 2.000000 | 34.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 19052.000000 | 18104.500000 | 17071.000000 | 2100.000000 | 2.009000e+03 | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | . 75% 22500.250000 | 240000.000000 | 2.000000 | 2.000000 | 2.000000 | 41.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 54506.000000 | 50190.500000 | 49198.250000 | 5006.000000 | 5.000000e+03 | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | . max 30000.000000 | 1000000.000000 | 2.000000 | 6.000000 | 3.000000 | 79.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | ... | 891586.000000 | 927171.000000 | 961664.000000 | 873552.000000 | 1.684259e+06 | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | . 8 rows × 25 columns . Initial observations . Slightly larger number of female&#39;s than male. | Majority of the people are either married or single | Average age of the customers is 35 years old. | Most of the customers are well educated i.e university graduates | Amount of limit balance varies quite a bit as the sd is quite large 0 - will have to look into this more closely later | In general people do not default on thier payments, however we want to focus on those who do default | . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to see what we are working with . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;AGE&#39;, &#39;PAY_1&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;DEFAULT&#39;, &#39;Education_higher&#39;, &#39;Education_university&#39;, &#39;Education_highschool&#39;, &#39;Marriage_Single&#39;, &#39;Marriage_Other&#39;], dtype=&#39;object&#39;) . credit_df . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5003 | 3047 | 5000 | 1000 | 0 | 0 | 0 | 1 | 0 | 0 | . 29996 29997 | 150000 | 0 | 43 | -1 | -1 | -1 | -1 | 0 | 0 | ... | 8998 | 129 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 29997 29998 | 30000 | 0 | 37 | 4 | 3 | 2 | -1 | 0 | 0 | ... | 22000 | 4200 | 2000 | 3100 | 1 | 0 | 1 | 0 | 1 | 0 | . 29998 29999 | 80000 | 0 | 41 | 1 | -1 | 0 | 0 | 0 | -1 | ... | 1178 | 1926 | 52964 | 1804 | 1 | 0 | 0 | 1 | 0 | 0 | . 29999 30000 | 50000 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1430 | 1000 | 1000 | 1000 | 1 | 0 | 1 | 0 | 0 | 0 | . 30000 rows × 28 columns . Target column - DEFAULT . Initial expectations . Looking at the target column we know it only has 2 values - default on payment next month? Yes or no. I am contructing a pie chart to see the distribution of the column. I expect the data to be leaning towards more non-defaulters i.e 0 values. As people do tend to pay thier credit card bills on time usually to avoid late fee&#39;s encurring or debt which can be exorbitant depending on company policy (unless there is a major economic event like recession). . However it is hard to say what percentage of a split we will see as it could be 60:40, 55:45, 70:30 etc. . colours = sns.color_palette(&#39;Set2&#39;)[0:5] (credit_df[&#39;DEFAULT&#39;].value_counts(normalize=True)*100).plot(kind=&#39;pie&#39;,autopct=&#39;%.0f%%&#39;,colors=colours,legend=True, figsize=(14,6),startangle=75) # plot using value counts and normalise as it shows us the distribution as a percentage of the whole column plt.legend([&#39;0:Not default&#39;,&#39;1: Default&#39;],loc=&#39;lower right&#39;) plt.title(&#39;Default on payment next month?&#39;) plt.show() . As we know from before that this is a very imbalanced dataset. This is normal as most people will have paid thier credit card bill on time. However I will have to do upsampling on the smaller class to avoid bias to the majority class, which can cause the number of false negatives predicted to be higher. . Demographic observations . The ID column is just an identifier, which also has no predictive value this is something we can remove later when modelling. . . &lt;AxesSubplot:xlabel=&#39;LIMIT_BAL&#39;, ylabel=&#39;Count&#39;&gt; . #credit_df[&#39;LIMIT_BAL&#39;].value_counts(5).plot(kind=&#39;bar&#39;) colors = sns.color_palette(&#39;rocket&#39;) (credit_df[&#39;LIMIT_BAL&#39;].value_counts(normalize=True)*100).head(5) . 50000 11.216667 20000 6.586667 30000 5.366667 80000 5.223333 200000 5.093333 Name: LIMIT_BAL, dtype: float64 . credit_df_clean[&#39;LIMIT_BAL&#39;].describe() . count 30000.000000 mean 167484.322667 std 129747.661567 min 10000.000000 25% 50000.000000 50% 140000.000000 75% 240000.000000 max 1000000.000000 Name: LIMIT_BAL, dtype: float64 . The most common credit balance limits are 50k, 20k and 30k respectively. Compared to the minimum 10k and maximum of 1000000 the figures are on the lower end of the range. However the maximum amount could be a outlier as its more than 2 standard deviations away from the mean. . limit_log=np.log(credit_df[&#39;LIMIT_BAL&#39;]) #limit_log.plot(kind=&#39;hist&#39;) # plt.subplots(1, 2, figsize = (9, 4)) plt.tight_layout() plt.subplot(1, 2, 1) sns.histplot(data=credit_df[&#39;LIMIT_BAL&#39;],bins=50,kde=True) plt.subplot(1, 2, 2) sns.histplot(limit_log, kde=True) . target = credit_df[&#39;DEFAULT&#39;] # Marriage, Age, and Sex def boxplot_comparison(feature1, feature2, feature3, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=feature1, y=feature2, hue=target, data=credit_df, palette=&#39;pastel&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_comparison(&#39;MARRIAGE_SINGLE&#39;, &#39;AGE&#39;, 10) . boxplot_comparison(&#39;EDUCATION_&#39;, &#39;LIMIT_BAL&#39;, 10) . for col in credit_df_clean.columns[4:22]: plt.subplots(1, 2, figsize = (10, 3)) plt.tight_layout() plt.subplot(1, 2, 1) plt.title(f&#39;Original: {col}&#39;) sns.histplot(credit_df_clean[col], bins = 40) plt.subplot(1, 2, 2) plt.title(f&#39;Log transformed: {col}&#39;) sns.histplot(credit_df_clean[col], bins = 40) plt.yscale(&#39;log&#39;) plt.show() . Correlation . corr = credit_df_clean.corr(method=&#39;spearman&#39;) mask = np.zeros_like(corr,dtype=bool) mask[np.triu_indices_from(mask)]=True cmap = sns.diverging_palette(230, 20, as_cmap=True) with sns.axes_style(&quot;white&quot;): plt.subplots(figsize=(18, 18)) sns.heatmap(corr, mask=mask, square=True, linewidths=.3, fmt=&#39;.2f&#39;, cmap=cmap, annot=True, annot_kws={&quot;size&quot;: 12}) plt.title(&#39;Correlation matrix&#39;, size=15) plt.show() #ref - https://seaborn.pydata.org/examples/many_pairwise_correlations.html . loads of correlated columns - will have to do PCA on this. notably with the default column limit bal is negatively correlated and pay_1 has the highest correlation followed by the res of the pay columns. .",
            "url": "https://apayal.github.io/capstone_project/2022/07/01/eda.html",
            "relUrl": "/2022/07/01/eda.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Predicting credit card defaults",
            "content": "Notebook 2 - EDA . Exploring the data . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns . credit_df=pd.read_csv(r&#39;C: Users Robin Downloads capstone_project _data taiwan_data.csv&#39;) . Overview of the data . credit_df.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | 2 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | 1 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 25 columns . credit_df.describe() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 3.000000e+04 | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 15000.500000 | 167484.322667 | 1.603733 | 1.853133 | 1.551867 | 35.485500 | -0.016700 | -0.133767 | -0.166200 | -0.220667 | ... | 43262.948967 | 40311.400967 | 38871.760400 | 5663.580500 | 5.921163e+03 | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | . std 8660.398374 | 129747.661567 | 0.489129 | 0.790349 | 0.521970 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | ... | 64332.856134 | 60797.155770 | 59554.107537 | 16563.280354 | 2.304087e+04 | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | . min 1.000000 | 10000.000000 | 1.000000 | 0.000000 | 0.000000 | 21.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | ... | -170000.000000 | -81334.000000 | -339603.000000 | 0.000000 | 0.000000e+00 | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 7500.750000 | 50000.000000 | 1.000000 | 1.000000 | 1.000000 | 28.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 2326.750000 | 1763.000000 | 1256.000000 | 1000.000000 | 8.330000e+02 | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | . 50% 15000.500000 | 140000.000000 | 2.000000 | 2.000000 | 2.000000 | 34.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 19052.000000 | 18104.500000 | 17071.000000 | 2100.000000 | 2.009000e+03 | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | . 75% 22500.250000 | 240000.000000 | 2.000000 | 2.000000 | 2.000000 | 41.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 54506.000000 | 50190.500000 | 49198.250000 | 5006.000000 | 5.000000e+03 | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | . max 30000.000000 | 1000000.000000 | 2.000000 | 6.000000 | 3.000000 | 79.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | ... | 891586.000000 | 927171.000000 | 961664.000000 | 873552.000000 | 1.684259e+06 | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | . 8 rows × 25 columns . Initial observations . Slightly larger number of female&#39;s than male. | Majority of the people are either married or single | Average age of the customers is 35 years old. | The PAY_ columns range is actually -2 to 8 whereas in the dictionary it said -1 to 9. | Most of the customers are well educated i.e university graduates | Amount of limit balance varies quite a bit as the sd is quite large | In general people do not default on thier payments, however we want to focus on those who do default | . credit_df.columns #pd.DataFrame(credit_df.columns) #looking at columns names to see what we are working with . Index([&#39;ID&#39;, &#39;LIMIT_BAL&#39;, &#39;SEX&#39;, &#39;EDUCATION&#39;, &#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;PAY_0&#39;, &#39;PAY_2&#39;, &#39;PAY_3&#39;, &#39;PAY_4&#39;, &#39;PAY_5&#39;, &#39;PAY_6&#39;, &#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;, &#39;PAY_AMT1&#39;, &#39;PAY_AMT2&#39;, &#39;PAY_AMT3&#39;, &#39;PAY_AMT4&#39;, &#39;PAY_AMT5&#39;, &#39;PAY_AMT6&#39;, &#39;default payment next month&#39;], dtype=&#39;object&#39;) . new_column_names = {&#39;PAY_0&#39;: &#39;PAY_1&#39;, &#39;default payment next month&#39;:&#39;DEFAULT&#39; } . credit_df.rename(columns=new_column_names,inplace=True) . credit_df.info() # check they have been changed . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 30000 entries, 0 to 29999 Data columns (total 25 columns): # Column Non-Null Count Dtype -- -- 0 ID 30000 non-null int64 1 LIMIT_BAL 30000 non-null int64 2 SEX 30000 non-null int64 3 EDUCATION 30000 non-null int64 4 MARRIAGE 30000 non-null int64 5 AGE 30000 non-null int64 6 PAY_1 30000 non-null int64 7 PAY_2 30000 non-null int64 8 PAY_3 30000 non-null int64 9 PAY_4 30000 non-null int64 10 PAY_5 30000 non-null int64 11 PAY_6 30000 non-null int64 12 BILL_AMT1 30000 non-null int64 13 BILL_AMT2 30000 non-null int64 14 BILL_AMT3 30000 non-null int64 15 BILL_AMT4 30000 non-null int64 16 BILL_AMT5 30000 non-null int64 17 BILL_AMT6 30000 non-null int64 18 PAY_AMT1 30000 non-null int64 19 PAY_AMT2 30000 non-null int64 20 PAY_AMT3 30000 non-null int64 21 PAY_AMT4 30000 non-null int64 22 PAY_AMT5 30000 non-null int64 23 PAY_AMT6 30000 non-null int64 24 DEFAULT 30000 non-null int64 dtypes: int64(25) memory usage: 5.7 MB . Lets focus on cleaning and checking for inconsistencies in the demographic columns of the dataset first. . 1. SEX . From the data dictionary we already know that sex has the values 1 and 2 representing male and female. However in computing binary values are traditionally represented as 0 and 1, so we will change them. . credit_df[&#39;SEX&#39;] . 0 2 1 2 2 2 3 2 4 1 .. 29995 1 29996 1 29997 1 29998 1 29999 1 Name: SEX, Length: 30000, dtype: int64 . credit_df[&#39;SEX&#39;]=credit_df[&#39;SEX&#39;].replace({1: 0, 2: 1}) . credit_df[&#39;SEX&#39;].value_counts() # check we only have 0 and 1 left . 1 18112 0 11888 Name: SEX, dtype: int64 . Education . Education also has some extra values. We already know there is 2 redundent values of 5 and 6. We have 4 to denote unknown or other education status. We can do a value counts to check if there are anymore unknown values and if there is also assign them to 4. . credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 # as suspected another unknown value - 0,not mentioned in description . 2 46.766667 1 35.283333 3 16.390000 5 0.933333 4 0.410000 6 0.170000 0 0.046667 Name: EDUCATION, dtype: float64 . credit_df[&#39;EDUCATION&#39;]=credit_df[&#39;EDUCATION&#39;].replace([0,5,6], 4 ) credit_df[&#39;EDUCATION&#39;].value_counts(normalize=True)*100 # now do get dummies so we only have 3 columns left . 2 46.766667 1 35.283333 3 16.390000 4 1.560000 Name: EDUCATION, dtype: float64 . There is actually 3 unknown values: 0,5,6. We can map these to the value 4 to refer to other either instituations or other unknown place of study. . However upon doing some external reseach, I found that Taiwan has a very high percentage of education completion in general. In fact around 94.7% of people have passed high school and gone onto some sort of higher education or senior vocational course . Based on this information, it is safe to assume that the unknown values and other category can be grouped together in the value 3, as it is highly likely that these people have passed high school at very the least. Additionally since adding the 3 unknown values to 4 gives us 468 values which accounts for less than 1.56% of the entire column it is insignificant in making predictions which means it can only contribute to making models more computationally expensive to compute. . education_df = pd.get_dummies(credit_df[&#39;EDUCATION&#39;],prefix=&#39;Education&#39;) # or undo this and just do a get dummies instead . education_df.drop(columns=[&#39;Education_4&#39;], inplace=True) . education_df . Education_1 Education_2 Education_3 . 0 0 | 1 | 0 | . 1 0 | 1 | 0 | . 2 0 | 1 | 0 | . 3 0 | 1 | 0 | . 4 0 | 1 | 0 | . ... ... | ... | ... | . 29995 0 | 0 | 1 | . 29996 0 | 0 | 1 | . 29997 0 | 1 | 0 | . 29998 0 | 0 | 1 | . 29999 0 | 1 | 0 | . 30000 rows × 3 columns . education_df.value_counts() . Education_1 Education_2 Education_3 0 1 0 14030 1 0 0 10585 0 0 1 4917 0 468 dtype: int64 . Marriage . This also has 1 unknown value of 0, which we can change to 3 (others) . credit_df[&#39;MARRIAGE&#39;].value_counts() . 2 15964 1 13659 3 323 0 54 Name: MARRIAGE, dtype: int64 . credit_df[&#39;MARRIAGE&#39;]=credit_df[&#39;MARRIAGE&#39;].replace(0,3) . credit_df[&#39;MARRIAGE&#39;].value_counts(normalize=True)*100 . 2 53.213333 1 45.530000 3 1.256667 Name: MARRIAGE, dtype: float64 . Married and single can essentially be represented as 1 columns of married or not. Although value 3 only accounts for 1.25% of the column we can at the moment have a separate column for it but later when we do PCA or feature engineering I suspect it will be unimportant in predictions ans thus will be removed or not included in the modelling. . credit_df[&#39;MARRIAGE&#39;] . 0 1 1 2 2 2 3 1 4 1 .. 29995 1 29996 2 29997 2 29998 1 29999 1 Name: MARRIAGE, Length: 30000, dtype: int64 . marriage_df=pd.get_dummies(credit_df[&#39;MARRIAGE&#39;],prefix=&#39;Marital_status&#39;,drop_first=True) . marriage_df . Marital_status_2 Marital_status_3 . 0 0 | 0 | . 1 1 | 0 | . 2 1 | 0 | . 3 0 | 0 | . 4 0 | 0 | . ... ... | ... | . 29995 0 | 0 | . 29996 1 | 0 | . 29997 1 | 0 | . 29998 0 | 0 | . 29999 0 | 0 | . 30000 rows × 2 columns . . credit_df_clean = pd.concat([credit_df,education_df, marriage_df], axis=1) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_1 Education_2 Education_3 Marital_status_2 Marital_status_3 . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . rename_col = {&#39;Education_1&#39;: &#39;Education_higher&#39;, &#39;Education_2&#39;:&#39;Education_university&#39;, &#39;Education_3&#39;:&#39;Education_highschool&#39;, &#39;Marital_status_2&#39;:&#39;Marriage_Single&#39;, &#39;Marital_status_3&#39;:&#39;Marriage_Other&#39; } . credit_df_clean.rename(columns=rename_col,inplace=True) credit_df_clean.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_1 PAY_2 PAY_3 PAY_4 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . 5 rows × 30 columns . credit_df_clean=credit_df_clean.drop(columns=[&#39;EDUCATION&#39;,&#39;MARRIAGE&#39;]) . credit_df_clean . ID LIMIT_BAL SEX AGE PAY_1 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 DEFAULT Education_higher Education_university Education_highschool Marriage_Single Marriage_Other . 0 1 | 20000 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1 2 | 120000 | 1 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 1000 | 1000 | 0 | 2000 | 1 | 0 | 1 | 0 | 1 | 0 | . 2 3 | 90000 | 1 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1000 | 1000 | 1000 | 5000 | 0 | 0 | 1 | 0 | 1 | 0 | . 3 4 | 50000 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1200 | 1100 | 1069 | 1000 | 0 | 0 | 1 | 0 | 0 | 0 | . 4 5 | 50000 | 0 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 10000 | 9000 | 689 | 679 | 0 | 0 | 1 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 29995 29996 | 220000 | 0 | 39 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 5003 | 3047 | 5000 | 1000 | 0 | 0 | 0 | 1 | 0 | 0 | . 29996 29997 | 150000 | 0 | 43 | -1 | -1 | -1 | -1 | 0 | 0 | ... | 8998 | 129 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | . 29997 29998 | 30000 | 0 | 37 | 4 | 3 | 2 | -1 | 0 | 0 | ... | 22000 | 4200 | 2000 | 3100 | 1 | 0 | 1 | 0 | 1 | 0 | . 29998 29999 | 80000 | 0 | 41 | 1 | -1 | 0 | 0 | 0 | -1 | ... | 1178 | 1926 | 52964 | 1804 | 1 | 0 | 0 | 1 | 0 | 0 | . 29999 30000 | 50000 | 0 | 46 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1430 | 1000 | 1000 | 1000 | 1 | 0 | 1 | 0 | 0 | 0 | . 30000 rows × 28 columns . Demographic observations . The ID column is just an identifier, which also has no predictive value this is something we can remove later when modelling. . plt.subplots(1, 2, figsize = (9, 4)) plt.tight_layout() plt.subplot(1, 2, 1) sns.histplot(data=credit_df[&#39;LIMIT_BAL&#39;],bins=50,kde=True) plt.subplot(1, 2, 2) sns.histplot(limit_log, kde=True) . &lt;AxesSubplot:xlabel=&#39;LIMIT_BAL&#39;, ylabel=&#39;Count&#39;&gt; . #credit_df[&#39;LIMIT_BAL&#39;].value_counts(5).plot(kind=&#39;bar&#39;) colors = sns.color_palette(&#39;rocket&#39;) credit_df[&#39;LIMIT_BAL&#39;].value_counts().head(3).plot(kind=&#39;bar&#39;,color=&#39;coral&#39;) . &lt;AxesSubplot:&gt; . credit_df_clean[&#39;LIMIT_BAL&#39;].describe() . count 30000.000000 mean 167484.322667 std 129747.661567 min 10000.000000 25% 50000.000000 50% 140000.000000 75% 240000.000000 max 1000000.000000 Name: LIMIT_BAL, dtype: float64 . limit_log=np.log(credit_df_clean[&#39;LIMIT_BAL&#39;]) limit_log.plot(kind=&#39;hist&#39;) #np.log(credit_df_clean[&#39;LIMIT_BAL&#39;]).plot(kind=&#39;hist&#39;) . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; . The most common credit balance limits are 50k, 20k and 30k respectively. Compared to the minimum 10k and maximum of 1000000 the figures are on the lower end of the range. However the maximum amount could be a outlier as its more than 2 standard deviations away from the mean. . # Marriage, Age, and Sex def boxplot_variation(feature1, feature2, feature3, width=16): fig, ax1 = plt.subplots(ncols=1, figsize=(width, 6)) s = sns.boxplot(ax=ax1, x=feature1, y=feature2, hue=feature3, data=credit_df, palette=&#39;pastel&#39;) #s.set_xticklabels(s.get_xticklabels(), rotation=90) plt.show(); boxplot_variation(&#39;MARRIAGE&#39;, &#39;AGE&#39;, &#39;SEX&#39;, 10) . boxplot_variation(&#39;EDUCATION&#39;, &#39;LIMIT_BAL&#39;, &#39;DEFAULT&#39;, 10) . credit_df_pay_col=credit_df_clean.columns[4:10] . for month in credit_df_pay_col: print(sorted(credit_df_clean[month].unique())) . [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8] [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8] [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8] [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8] [-2, -1, 0, 2, 3, 4, 5, 6, 7, 8] [-2, -1, 0, 2, 3, 4, 5, 6, 7, 8] . # These ranges are not mentioned in the description, let&#39;s change the range to back to -1-9? for month in credit_df_clean.columns[4:10]: credit_df_clean[month]=credit_df_clean[month]+1 # inplace=True) credit_df_clean[&#39;PAY_1&#39;].value_counts() . 1 14737 0 5686 2 3688 -1 2759 3 2667 4 322 5 76 6 26 9 19 7 11 8 9 Name: PAY_1, dtype: int64 . credit_df[&#39;BILL_AMT1&#39;].value_counts().sort_values() . 4984 1 133572 1 24132 1 114227 1 30415 1 ... 316 63 326 72 780 76 390 244 0 2008 Name: BILL_AMT1, Length: 22723, dtype: int64 . for col in credit_df_clean.columns[4:22]: plt.subplots(1, 2, figsize = (10, 3)) plt.tight_layout() plt.subplot(1, 2, 1) plt.title(f&#39;Original: {col}&#39;) sns.histplot(credit_df_clean[col], bins = 40) plt.subplot(1, 2, 2) plt.title(f&#39;Log transformed: {col}&#39;) sns.histplot(credit_df_clean[col], bins = 40) plt.yscale(&#39;log&#39;) plt.show() . Target column - default . colours = sns.color_palette(&#39;Set2&#39;)[0:5] (credit_df[&#39;DEFAULT&#39;].value_counts(normalize=True)*100).plot(kind=&#39;pie&#39;,autopct=&#39;%.0f%%&#39;,colors=colours,legend=True, figsize=(14,6),startangle=75) plt.legend([&#39;0:Not default&#39;,&#39;1: Default&#39;],loc=&#39;lower right&#39;) plt.title(&#39;Default on payment next month?&#39;) plt.show() . # v=credit_df[&#39;DEFAULT&#39;].value_counts(normalize=True)*100 # fig= px.pie(credit_df) # fig.show() . Very imbalanced dataset. This is normal as most people will have paid thier credit card bill on time. However I will have to do upsampling on the smaller class to avoid bias to the majority class, which can cause the number of false negatives predicted to be higher. . #credit_df.iloc[:,12:25] plt.figure(figsize=(10,7)) cmap = sns.diverging_palette(230, 20, as_cmap=True) sns.heatmap(credit_df.iloc[:,12:25].corr(),annot=True,cmap=cmap) . &lt;AxesSubplot:&gt; . corr = credit_df_clean.corr(method=&#39;spearman&#39;) mask = np.zeros_like(corr,dtype=bool) mask[np.triu_indices_from(mask)]=True cmap = sns.diverging_palette(230, 20, as_cmap=True) with sns.axes_style(&quot;white&quot;): plt.subplots(figsize=(18, 18)) sns.heatmap(corr, mask=mask, square=True, linewidths=.3, fmt=&#39;.2f&#39;, cmap=cmap, annot=True, annot_kws={&quot;size&quot;: 12}) plt.title(&#39;Correlation matrix&#39;, size=15) plt.show() #ref - https://seaborn.pydata.org/examples/many_pairwise_correlations.html . loads of correlated columns - will have to do PCA on this. notably with the default column limit bal is negatively correlated and pay_1 has the highest correlation followed by the res of the pay columns. . credit_df_clean.to_csv(&#39;credit-df-dataset-cleaned.csv&#39;, index=False) # the index column will not be saved .",
            "url": "https://apayal.github.io/capstone_project/2022/07/01/eda-Copy.html",
            "relUrl": "/2022/07/01/eda-Copy.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://apayal.github.io/capstone_project/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://apayal.github.io/capstone_project/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://apayal.github.io/capstone_project/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://apayal.github.io/capstone_project/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}